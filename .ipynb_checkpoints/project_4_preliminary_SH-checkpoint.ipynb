{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Final Project - Random Acts of Pizza\n",
    "### Predicting altruism through free pizza\n",
    "\n",
    "This project is originated from the Kaggle competition https://www.kaggle.com/c/random-acts-of-pizza. We will create an algorithm to predict which requests will recieve pizza and which on will not.  The competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. This data was collected and graciously shared by Althoff et al (http://www.timalthoff.com/). \n",
    "\n",
    "**Reference Paper:**\n",
    "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014. (http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach \n",
    "\n",
    "\n",
    "**Step 1:  Exploratory Data Analysis **\n",
    "\n",
    "**Step 2:  Create a Baseline Model **\n",
    "\n",
    "**Step 3:  Feature Engineering **\n",
    "\n",
    "- Preprocessing data \n",
    "    - data cleansing \n",
    "    - data transformation\n",
    "\n",
    "- Use other meta-data from the data set, such as,\n",
    "    - request_text_edit_aware\n",
    "    - request_title\n",
    "    - requester_account_age_in_days_at_request\n",
    "    - requester_days_since_first_post_on_raop_at_request\n",
    "    - requester_number_of_comments_at_request\n",
    "    - requester_number_of_comments_in_raop_at_request\n",
    "    - requester_number_of_posts_at_request\n",
    "    - requester_number_of_posts_on_raop_at_request\n",
    "    - requester_number_of_subreddits_at_request\n",
    "    - requester_subreddits_at_request\n",
    "    - requester_upvotes_minus_downvotes_at_request\n",
    "    - requester_upvotes_plus_downvotes_at_request\n",
    "    - requester_username\n",
    "    - unix_timestamp_of_request\n",
    "    - unix_timestamp_of_request_utc\n",
    "    - other features includes:\n",
    "        - number of requests made by the same user\n",
    "        - number of requests fulfilled or % of requests fulfilled, etc\n",
    "    \n",
    "- Generate new features from the data set such as, \n",
    "    - Politeness, \n",
    "    - Evidentiality, \n",
    "    - Reciprocity, \n",
    "    - Sentiment, \n",
    "    - Length, etc\n",
    "\n",
    "**Step 4:  Algorithm / Model Selection **\n",
    "\n",
    "- Generative Models \n",
    "    - Naive Bayes \n",
    "- Discriminative Models\n",
    "    - Logistic Regression  \n",
    "- Neural Network \n",
    "\n",
    "\n",
    "**Step 5:  Error Analysis & Optimization ** \n",
    "\n",
    "\n",
    "**Step 6:  Final Model ** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# SK-learn libraries for model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# json libraries to parse json file\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4040, 32)\n",
      "Test shape:  (1631, 17)\n"
     ]
    }
   ],
   "source": [
    "# read json file\n",
    "train_json = json.load(open('/Users/shanhe/Desktop/W207/Final Project/EDA/train.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "train_json_df = json_normalize(train_json)\n",
    "\n",
    "# read json file\n",
    "test_json = json.load(open('/Users/shanhe/Desktop/W207/Final Project/EDA/test.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "test_json_df = json_normalize(test_json)\n",
    "\n",
    "print(\"Train shape: \", train_json_df.shape)\n",
    "print(\"Test shape: \", test_json_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 32 columns, while the test data only has 17. __Data providers have removed fields from the test set which would not be available at the time of posting.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataframe with only the mathcing train columns\n",
    "test_w_train_col = train_json_df[test_json_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    3046\n",
      "True      994\n",
      "Name: requester_received_pizza, dtype: int64\n",
      "Percentage of request failures: 75.0%.\n",
      "Percentage of request failures: 25.0%.\n"
     ]
    }
   ],
   "source": [
    "# Check to see that dataset is balanced in number of request success and failures\n",
    "print(train_json_df['requester_received_pizza'].value_counts())\n",
    "print(\"Percentage of request failures: \" + str(round(3046/len(train_json_df),2)*100)+\"%.\")\n",
    "print(\"Percentage of request failures: \" + str(round(994/len(train_json_df),2)*100)+\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should consider balancing our features to that it's closer to 50-50. To do this we can try duplicating our successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 17)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w_train_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (3232, 17)\n",
      "dev data shape: (808, 17)\n",
      "test data shape: (1631, 17)\n",
      "training labels shape: (3232,)\n",
      "dev labels shape: (808,)\n"
     ]
    }
   ],
   "source": [
    "# for the baseline, we just use the \"request_text_edit_aware\" as input features \n",
    "# but we can explore other metadata (such as \"request_title\", \n",
    "# \"number_of_downvotes_of_request_at_retrieval\", \"number_of_upvotes_of_request_at_retrieval\", etc) \n",
    "# to add to the input features \n",
    "# train_data = train_json_df.request_text_edit_aware.as_matrix()\n",
    "\n",
    "# convert the requester_received_pizza field to 0 and 1\n",
    "# 0 means the user doesn't receive pizza & 1 means the user receives pizza\n",
    "train_labels = train_json_df.requester_received_pizza.astype(int).as_matrix()\n",
    "\n",
    "# split the training data into training data and dev data \n",
    "train_data_df, dev_data_df, train_labels, dev_labels = \\\n",
    "            train_test_split(test_w_train_col, train_labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "# apply same logic as train_data to test_data\n",
    "test_data_df = test_json_df\n",
    "\n",
    "print('training data shape:', train_data_df.shape)\n",
    "print('dev data shape:', dev_data_df.shape)\n",
    "print('test data shape:', test_data_df.shape)\n",
    "print('training labels shape:', train_labels.shape)\n",
    "print('dev labels shape:', dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "This message doesn't receive pizza : Sample 1\n",
      "-----------------------------------------------\n",
      "My power was out for about 3 hours earlier this afternoon. I keep trying to watch DVD's (Twister...I mean when in Rome, right?) but as soon as I get halfway in the power either flickers on and off or stays off for an extended period of time. I don't feel up to going out for food either since I've been sick for about 3 days now and Hurricane Irene is being a bitch...\n",
      "\n",
      "Thank you all!\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message doesn't receive pizza : Sample 2\n",
      "-----------------------------------------------\n",
      "I'm lucky that internet comes part and parcel with my rent. I work retail grocery and summer hours are murder on the wallet. I have to hustle managers just to get a good 20-25 hours a week, when I used to push the part time hour limit. \n",
      "\n",
      "I hate begging, but my stomach is scoffing at me and calling me a proud asshole. So here I am :). Just one is all I ask.\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message doesn't receive pizza : Sample 3\n",
      "-----------------------------------------------\n",
      "Hey RAoP\n",
      "\n",
      "This is a throwaway account but I'd be happy to PM you from my real account as verification. I've given before and never received.\n",
      "\n",
      "First up I'd like to say that pizza is NOT necessary and we won't starve without it. Not even a bit. We've got some leftovers in the freezer and a bunch of odds and ends in the cupboards and even a bunch of friends who'd take us out to dinner or have us over if we asked (but we'd feel bad asking because we're old enough to be meant to be beyond this!). So, absolutely no worries if no-one can give. \n",
      "\n",
      "I'm asking because a bunch of unexpected grown up expenses just ambushed us. My boyfriend's grandfather died unexpectedly and we wanted to go to the funeral - but it was 5 hours away and we didn't have a car. So we bought a cheap car and made it to the funeral, but it turns out that car ownership is super expensive. Rego is nearly $1000! Add insurance, NRMA membership (needed cos we weren't sure whether our little bomb of a car would make the drive and we didnt want to be stranded in the australian outback forever), random fees for vehicle inspections and more random fees just for existing add up to be the value of the car again! What. The. Hell. (yes, I *have* lived a sheltered life, thank you for asking.)\n",
      "\n",
      "Add that to the billion and one other things we're saving up for/need to pay (rent, bills, house deposit, wedding, honeymoon maybe, helping our families) and it's come as a bit of a shock to our financial system :P\n",
      "We literally have $0.67 in the bank and a bit of change in cash until pay day. This is an odd and disconcerting feeling. \n",
      "\n",
      "What would pizza mean to us? Laughter and joy. The pleasure of knowing that there are good people who care about strangers. A memory to carry with us both when things are hard and when things are good and we can give again. A small easing of financial stress for the next few days. \n",
      "\n",
      "I might also pull the request later because like I said - it's most definitely not a do or die type situation. And anyway, my credit card's taken such a pounding this week, what harm is pizza going to do? :P\n",
      "\n",
      "But thanks for reading anyway, RAoP!\n",
      "\n",
      "Also, please and thank you ;)\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message doesn't receive pizza : Sample 4\n",
      "-----------------------------------------------\n",
      "I'm in Covington.  If anyone would give the gift of pizza, I'd be most most appreciative.  I'm a broke writer who only had ramen in his house.  Thanks!\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message doesn't receive pizza : Sample 5\n",
      "-----------------------------------------------\n",
      "I could really use a pizza right now.  It's my friend and I, and we have nothing to eat / no money.. and his car is in the shop.  So if one of you fine gentlemen , or fine ladies, could give us delicious, hot, circular-shaped heaven.. We would be most grateful.\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message receives pizza : Sample 1\n",
      "-----------------------------------------------\n",
      "Posting from my phone (contract expires next Wednesday) as we have no wifi. Girlfriend and I met on the Internet, have been in a long-distance relationship (with monthly visits) for ten months. Just moved into a new apartment with her last week. I'm the happiest I've been in a long time, but we are pretty tight on money. I'm job-hunting, she works at a minimum wage fast food restaurant (she brings food back from her job, which is how we eat right now). A pizza or two would be nice if anyone has a few extra bucks lying around. We can handle the delivery guy's tip. Thanks. \n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message receives pizza : Sample 2\n",
      "-----------------------------------------------\n",
      "Biochemistry is a hell of a major. \n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message receives pizza : Sample 3\n",
      "-----------------------------------------------\n",
      "So I made this throwaway because I just feel bad about having to post here, but it's a last resort.  I've been unemployed for a few months and my cash is just winding low.  I'm in the midst of classes that have been taking up my time and preventing me from finding a new job and I'm starting to really stress out about having to pay rent in ~10 days.  I'm not expecting anything, but I figured I'd try for a pizza.  I have a verification code.\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message receives pizza : Sample 4\n",
      "-----------------------------------------------\n",
      "I'm a full time student that also works full time, which means I never get time off! I'm enjoying the first day off I've had in over 3 months, but I have no food in the fridge and I don't get paid til wed! I'd be so grateful for a pizza right now &lt;3&lt;3\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "This message receives pizza : Sample 5\n",
      "-----------------------------------------------\n",
      "Hi guys, never done this before so I don't know if it will work or not but worth a shot is it not? Basically the situation is as follows.. I am in Wellington, New Zealand. I am hungover because last night I was out in town drinking and watching the warriors lose to manly in the NRL grand final :( used up most of my spare money taxing home and now I am looking at a fairly empty fridge/pantry.. My car is 30 mins walk away and it is PISSING down with rain which I don't really wanna brave to go to the super market.\n",
      "\n",
      "Pretty much wondering if a kickass random stranger would hook me up with a pizza to cure my hangover and cheer me up after yesterdays loss. Will be sure to pay it on in the future to another needy individual.\n",
      "\n",
      "Thankyou THR - you are the greatest!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples=5\n",
    "    \n",
    "# for each label, display a number of examples \n",
    "for i in range(2):\n",
    "\n",
    "    # find the indexes of the corresponding label \n",
    "    index = np.where(train_labels == i)\n",
    "\n",
    "    for j in range(num_examples):\n",
    "\n",
    "        # print the training data for that label\n",
    "        if i == 1:\n",
    "            title = \"This message receives pizza\"`d\n",
    "        else:\n",
    "            title = \"This message doesn't receive pizza\"\n",
    "        print(\"-----------------------------------------------\" )\n",
    "        print(\"{} : Sample {}\".format(title, j+1))\n",
    "        print(\"-----------------------------------------------\")\n",
    "        print(train_data[index[0][j]])\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 8.385454545454545}\n",
      "auc score using CountVectorizer & MultinomialNB = 0.5880349615375456\n"
     ]
    }
   ],
   "source": [
    "# use standard CountVectorizer to transform the training data and dev data \n",
    "vectorizer = CountVectorizer() \n",
    "train_bag_of_words = vectorizer.fit_transform(train_data_df.request_text_edit_aware.as_matrix())\n",
    "dev_bag_of_words = vectorizer.transform(dev_data_df.request_text_edit_aware.as_matrix())\n",
    "\n",
    "# create MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "    \n",
    "# test the best value for alpha\n",
    "parameters = {'alpha': np.linspace(0.01, 10, 100)}\n",
    "\n",
    "# create GridSearchCV to find the best alpha\n",
    "clf = GridSearchCV(nb, parameters)\n",
    "    \n",
    "# train the MultinomialNB\n",
    "clf.fit(train_bag_of_words, train_labels)\n",
    "\n",
    "pred_dev_prob = clf.predict_proba(dev_bag_of_words)[:,0]\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(\"auc score using CountVectorizer & MultinomialNB = {}\".format(roc_auc_score(dev_labels, pred_dev_prob, average='micro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Naive Bayes model combined with a bag of words approach as our baseline since  Naive Bayes is the most straightforward modeling approach to take in terms of model assumptions (feature independence) and complexity but also tends to work well with text. Word counts are natural features for NB since we calculate probabilities from observation counts. We see with an auc of .588, our baseline does better than randomly guessing (auc of .5) but not by too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>requester_days_since_first_post_on_raop_at_request</th>\n",
       "      <th>requester_number_of_comments_at_request</th>\n",
       "      <th>requester_number_of_comments_in_raop_at_request</th>\n",
       "      <th>requester_number_of_posts_at_request</th>\n",
       "      <th>requester_number_of_posts_on_raop_at_request</th>\n",
       "      <th>requester_number_of_subreddits_at_request</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>42.083866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>[AskReddit, COents, Denver, DenverBroncos, Lib...</td>\n",
       "      <td>364</td>\n",
       "      <td>840</td>\n",
       "      <td>j_like</td>\n",
       "      <td>1.308963e+09</td>\n",
       "      <td>1.308960e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>223.784537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>[Android, AskReddit, GrandTheftAutoV, IAmA, Mi...</td>\n",
       "      <td>516</td>\n",
       "      <td>1448</td>\n",
       "      <td>0110110101101100</td>\n",
       "      <td>1.379264e+09</td>\n",
       "      <td>1.379260e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>singledad22601</td>\n",
       "      <td>1.318636e+09</td>\n",
       "      <td>1.318633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>481.311273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>[AdviceAnimals, Art, AskReddit, GetMotivated, ...</td>\n",
       "      <td>1058</td>\n",
       "      <td>2062</td>\n",
       "      <td>Neuronut</td>\n",
       "      <td>1.375220e+09</td>\n",
       "      <td>1.375217e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>so_damn_hungry</td>\n",
       "      <td>1.335934e+09</td>\n",
       "      <td>1.335931e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_pvojb</td>\n",
       "      <td>So, I'm a student in London, and it's my birth...</td>\n",
       "      <td>[REQUEST] It's my birthday tomorrow (UK)</td>\n",
       "      <td>144.875093</td>\n",
       "      <td>44.114606</td>\n",
       "      <td>418</td>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>[Art, AskReddit, FIFA12, FantasyPL, IAmA, Life...</td>\n",
       "      <td>6331</td>\n",
       "      <td>31919</td>\n",
       "      <td>leiferic</td>\n",
       "      <td>1.329602e+09</td>\n",
       "      <td>1.329602e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_142n4c</td>\n",
       "      <td>I'm not entirely sure why, I guess just kindof...</td>\n",
       "      <td>[Request] Just kindof sad/disappointed, could ...</td>\n",
       "      <td>185.766100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[Random_Acts_Of_Pizza, atheism, technology]</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>cafepressguy</td>\n",
       "      <td>1.354313e+09</td>\n",
       "      <td>1.354313e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_17rja6</td>\n",
       "      <td>I'm a visiting medical student from Costa Rica...</td>\n",
       "      <td>[Request] Visiting student could use warm food.</td>\n",
       "      <td>1198.620231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[AnimalPorn, AskReddit, Music, Random_Acts_Of_...</td>\n",
       "      <td>176</td>\n",
       "      <td>286</td>\n",
       "      <td>javilopez1</td>\n",
       "      <td>1.359832e+09</td>\n",
       "      <td>1.359832e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1lg6u2</td>\n",
       "      <td>My SO and I are moving to the new apartment to...</td>\n",
       "      <td>[Request] Pregnant, packing @ 2am to move tomo...</td>\n",
       "      <td>43.332118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[IAmA]</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>RosyGraymalkin</td>\n",
       "      <td>1.377933e+09</td>\n",
       "      <td>1.377930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1b0mtx</td>\n",
       "      <td>My partner is a wonderful gender-queer pansexu...</td>\n",
       "      <td>[Request] My partner and I hit six-months, we ...</td>\n",
       "      <td>787.698322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>625</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>[Anarchism, AnarchistNews, Anarcho_Capitalism,...</td>\n",
       "      <td>2796</td>\n",
       "      <td>5186</td>\n",
       "      <td>vomitisjustskimmilk</td>\n",
       "      <td>1.364269e+09</td>\n",
       "      <td>1.364265e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known request_id  \\\n",
       "0                     N/A   t3_i8iy4   \n",
       "1                     N/A  t3_1mfqi0   \n",
       "2                     N/A   t3_lclka   \n",
       "3                     N/A  t3_1jdgdj   \n",
       "4                     N/A   t3_t2qt4   \n",
       "5                     N/A   t3_pvojb   \n",
       "6                     N/A  t3_142n4c   \n",
       "7                     N/A  t3_17rja6   \n",
       "8                     N/A  t3_1lg6u2   \n",
       "9                     N/A  t3_1b0mtx   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hey all! It's about 95 degrees here and our ki...   \n",
       "1  I didn't know a place like this exists! \\n\\nI ...   \n",
       "2  Hi Reddit. Im a single dad having a really rou...   \n",
       "3  Hi I just moved to Waltham MA from my home sta...   \n",
       "4  We're just sitting here near indianapolis on o...   \n",
       "5  So, I'm a student in London, and it's my birth...   \n",
       "6  I'm not entirely sure why, I guess just kindof...   \n",
       "7  I'm a visiting medical student from Costa Rica...   \n",
       "8  My SO and I are moving to the new apartment to...   \n",
       "9  My partner is a wonderful gender-queer pansexu...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0  [request] pregger gf 95 degree house and no fo...   \n",
       "1  [Request] Lost my job day after labour day, st...   \n",
       "2                (Request) pizza for my kids please?   \n",
       "3  [Request] Just moved to a new state(Waltham MA...   \n",
       "4  [Request] Two girls in between paychecks, we'v...   \n",
       "5           [REQUEST] It's my birthday tomorrow (UK)   \n",
       "6  [Request] Just kindof sad/disappointed, could ...   \n",
       "7    [Request] Visiting student could use warm food.   \n",
       "8  [Request] Pregnant, packing @ 2am to move tomo...   \n",
       "9  [Request] My partner and I hit six-months, we ...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  \\\n",
       "0                                 42.083866   \n",
       "1                                223.784537   \n",
       "2                                  0.000000   \n",
       "3                                481.311273   \n",
       "4                                  0.000000   \n",
       "5                                144.875093   \n",
       "6                                185.766100   \n",
       "7                               1198.620231   \n",
       "8                                 43.332118   \n",
       "9                                787.698322   \n",
       "\n",
       "   requester_days_since_first_post_on_raop_at_request  \\\n",
       "0                                           0.000000    \n",
       "1                                           0.000000    \n",
       "2                                           0.000000    \n",
       "3                                           0.000000    \n",
       "4                                           0.000000    \n",
       "5                                          44.114606    \n",
       "6                                           0.000000    \n",
       "7                                           0.000000    \n",
       "8                                           0.000000    \n",
       "9                                           0.000000    \n",
       "\n",
       "   requester_number_of_comments_at_request  \\\n",
       "0                                       57   \n",
       "1                                      145   \n",
       "2                                        0   \n",
       "3                                      277   \n",
       "4                                        0   \n",
       "5                                      418   \n",
       "6                                        2   \n",
       "7                                       42   \n",
       "8                                        1   \n",
       "9                                      625   \n",
       "\n",
       "   requester_number_of_comments_in_raop_at_request  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "5                                                2   \n",
       "6                                                0   \n",
       "7                                                0   \n",
       "8                                                0   \n",
       "9                                                0   \n",
       "\n",
       "   requester_number_of_posts_at_request  \\\n",
       "0                                    10   \n",
       "1                                    36   \n",
       "2                                     0   \n",
       "3                                    17   \n",
       "4                                     0   \n",
       "5                                   117   \n",
       "6                                     2   \n",
       "7                                    11   \n",
       "8                                     0   \n",
       "9                                   129   \n",
       "\n",
       "   requester_number_of_posts_on_raop_at_request  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "5                                             0   \n",
       "6                                             0   \n",
       "7                                             0   \n",
       "8                                             0   \n",
       "9                                             0   \n",
       "\n",
       "   requester_number_of_subreddits_at_request  \\\n",
       "0                                         16   \n",
       "1                                         29   \n",
       "2                                          0   \n",
       "3                                         30   \n",
       "4                                          0   \n",
       "5                                         41   \n",
       "6                                          3   \n",
       "7                                         14   \n",
       "8                                          1   \n",
       "9                                         72   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0  [AskReddit, COents, Denver, DenverBroncos, Lib...   \n",
       "1  [Android, AskReddit, GrandTheftAutoV, IAmA, Mi...   \n",
       "2                                                 []   \n",
       "3  [AdviceAnimals, Art, AskReddit, GetMotivated, ...   \n",
       "4                                                 []   \n",
       "5  [Art, AskReddit, FIFA12, FantasyPL, IAmA, Life...   \n",
       "6        [Random_Acts_Of_Pizza, atheism, technology]   \n",
       "7  [AnimalPorn, AskReddit, Music, Random_Acts_Of_...   \n",
       "8                                             [IAmA]   \n",
       "9  [Anarchism, AnarchistNews, Anarcho_Capitalism,...   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                           364   \n",
       "1                                           516   \n",
       "2                                             0   \n",
       "3                                          1058   \n",
       "4                                             0   \n",
       "5                                          6331   \n",
       "6                                             7   \n",
       "7                                           176   \n",
       "8                                             0   \n",
       "9                                          2796   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request   requester_username  \\\n",
       "0                                          840               j_like   \n",
       "1                                         1448     0110110101101100   \n",
       "2                                            0       singledad22601   \n",
       "3                                         2062             Neuronut   \n",
       "4                                            0       so_damn_hungry   \n",
       "5                                        31919             leiferic   \n",
       "6                                           27         cafepressguy   \n",
       "7                                          286           javilopez1   \n",
       "8                                            6       RosyGraymalkin   \n",
       "9                                         5186  vomitisjustskimmilk   \n",
       "\n",
       "   unix_timestamp_of_request  unix_timestamp_of_request_utc  \n",
       "0               1.308963e+09                   1.308960e+09  \n",
       "1               1.379264e+09                   1.379260e+09  \n",
       "2               1.318636e+09                   1.318633e+09  \n",
       "3               1.375220e+09                   1.375217e+09  \n",
       "4               1.335934e+09                   1.335931e+09  \n",
       "5               1.329602e+09                   1.329602e+09  \n",
       "6               1.354313e+09                   1.354313e+09  \n",
       "7               1.359832e+09                   1.359832e+09  \n",
       "8               1.377933e+09                   1.377930e+09  \n",
       "9               1.364269e+09                   1.364265e+09  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    }
   ],
   "source": [
    "def clean_datetime (df):\n",
    "    #clean up datetime\n",
    "    df.loc[:,'datetime_request'] = pd.to_datetime(df.unix_timestamp_of_request.map(lambda x: format(x, 'f')), unit = 's')\n",
    "    df.loc[:,'datetime_request_utc'] = pd.to_datetime(df.unix_timestamp_of_request_utc.map(lambda x: format(x, 'f')), unit = 's')\n",
    "    #build local month, week, day, hour\n",
    "    df.loc[:,'month_request'] = df['datetime_request'].dt.month\n",
    "    df.loc[:,'week_request'] = df['datetime_request'].dt.week\n",
    "    df.loc[:,'weekday_request'] = df['datetime_request'].dt.weekday\n",
    "    df.loc[:,'hour_request'] = df['datetime_request'].dt.hour\n",
    "    \n",
    "clean_datetime(train_data_df)\n",
    "clean_datetime(dev_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "giver_username_if_known                                       object\n",
       "request_id                                                    object\n",
       "request_text_edit_aware                                       object\n",
       "request_title                                                 object\n",
       "requester_account_age_in_days_at_request                     float64\n",
       "requester_days_since_first_post_on_raop_at_request           float64\n",
       "requester_number_of_comments_at_request                        int64\n",
       "requester_number_of_comments_in_raop_at_request                int64\n",
       "requester_number_of_posts_at_request                           int64\n",
       "requester_number_of_posts_on_raop_at_request                   int64\n",
       "requester_number_of_subreddits_at_request                      int64\n",
       "requester_subreddits_at_request                               object\n",
       "requester_upvotes_minus_downvotes_at_request                   int64\n",
       "requester_upvotes_plus_downvotes_at_request                    int64\n",
       "requester_username                                            object\n",
       "unix_timestamp_of_request                                    float64\n",
       "unix_timestamp_of_request_utc                                float64\n",
       "datetime_request                                      datetime64[ns]\n",
       "datetime_request_utc                                  datetime64[ns]\n",
       "week_request                                                   int64\n",
       "weekday_request                                                int64\n",
       "hour_request                                                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
       "       'request_title', 'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc',\n",
       "       'datetime_request', 'datetime_request_utc', 'week_request',\n",
       "       'weekday_request', 'hour_request'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date time dummies, for models that need dummy variables\n",
    "def dt_dummies(df):    \n",
    "    week_dummies = pd.get_dummies(df['week_request'], prefix  = 'week')\n",
    "    weekday_dummies = pd.get_dummies(df['weekday_request'], prefix  = 'weekday')\n",
    "    hour_dummies = pd.get_dummies(df['hour_request'], prefix  = 'hour')\n",
    "    \n",
    "    #return merged datetime features\n",
    "    return week_dummies.merge(hour_dummies.merge(weekday_dummies, left_index = True, right_index = True), left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard CountVectorizer to transform the text\n",
    "vectorizer = CountVectorizer() \n",
    "train_bag_of_words = vectorizer.fit_transform(train_data_df.request_text_edit_aware)\n",
    "dev_bag_of_words = vectorizer.transform(dev_data_df.request_text_edit_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard CountVectorizer with defined processor to transform the text\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    \n",
    "    #turn all characters to lowers case and tokenize all numbers to one token\n",
    "    pattern = re.compile(r'\\d+')\n",
    "\n",
    "    #delete stop words\n",
    "    en_stop = get_stop_words('en') #get english stopwords\n",
    "    new_s = ' '.join([i for i in s.split() if not i in en_stop])\n",
    "    \n",
    "    #stemming\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    new_s = ' '.join([p_stemmer.stem(i) for i in new_s.split()])\n",
    "    \n",
    "#     #shortening long words, the choice of 6 characters came from reiteration to find the number with best performance\n",
    "#     new_s = ' '.join([w[:6] if len(w) > 6 else w for w in new_s.split()])\n",
    "    \n",
    "    return new_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_data_df.request_text_edit_aware[1610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My power was out for about 3 hours earlier this afternoon. I keep trying to watch DVD's (Twister...I mean when in Rome, right?) but as soon as I get halfway in the power either flickers on and off or stays off for an extended period of time. I don't feel up to going out for food either since I've been sick for about 3 days now and Hurricane Irene is being a bitch...\\n\\nThank you all!\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"power 999 hour earlier afternoon. keep tri watch dvd' (twister...i mean rome, right?) soon get halfway power either flicker stay extend period time. feel go food either sinc sick 999 day now hurrican iren bitch... thank all!\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = better_preprocessor(test)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3232, 10235)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing using word count + date time features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 3.037272727272727}\n",
      "auc score using CountVectorizer & MultinomialNB = 0.5724356379051656\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "train_set = sparse.hstack((train_bag_of_words\n",
    "                           , dt_dummies(train_data_df)\n",
    "                          ))\n",
    "\n",
    "dev_set = sparse.hstack((dev_bag_of_words\n",
    "                         , dt_dummies(dev_data_df)\n",
    "                        ))\n",
    "\n",
    "# create MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "    \n",
    "# test the best value for alpha\n",
    "parameters = {'alpha': np.linspace(0.01, 10, 100)}\n",
    "\n",
    "# create GridSearchCV to find the best alpha\n",
    "clf = GridSearchCV(nb, parameters)\n",
    "    \n",
    "# # train the MultinomialNB\n",
    "clf.fit(train_set, train_labels)\n",
    "\n",
    "pred_dev_prob = clf.predict_proba(dev_set)[:,0]\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(\"auc score using CountVectorizer & MultinomialNB = {}\".format(roc_auc_score(dev_labels, pred_dev_prob, average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n",
      "auc score using CountVectorizer & Logistic Regression = 0.4232060777822763\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "train_set = sparse.hstack((train_bag_of_words\n",
    "                           , dt_dummies(train_data_df)\n",
    "                          ))\n",
    "\n",
    "dev_set = sparse.hstack((dev_bag_of_words\n",
    "                         , dt_dummies(dev_data_df)\n",
    "                        ))\n",
    "\n",
    "# create LR \n",
    "LR_0 = LogisticRegression()\n",
    "    \n",
    "# test the best value for C\n",
    "parameters = {'C': np.linspace(0.01, 10, 100)}\n",
    "\n",
    "# create GridSearchCV to find the best C\n",
    "clf = GridSearchCV(LR_0, parameters)\n",
    "    \n",
    "# # train the MultinomialNB\n",
    "clf.fit(train_set, train_labels)\n",
    "\n",
    "pred_dev_prob = clf.predict_proba(dev_set)[:,0]\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(\"auc score using CountVectorizer & Logistic Regression = {}\".format(roc_auc_score(dev_labels, pred_dev_prob, average='micro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__XGBoost with default request text + other features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#features to be used by XGBoost\n",
    "train_feature = train_data_df[[\n",
    "       'requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request',\n",
    "#        'requester_subreddits_at_request',\n",
    "       'requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request', 'month_request',\n",
    "       'week_request','weekday_request', 'hour_request']]\n",
    "\n",
    "#add new features\n",
    "train_feature['length_of_request'] = train_data_df['request_text_edit_aware'].map(lambda x: len(x))\n",
    "train_feature['length_of_title'] = train_data_df['request_title'].map(lambda x: len(x))\n",
    "\n",
    "#inspired by the paper, How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Tim Althoff et al\n",
    "train_feature['first_half_of_month'] = train_data_df['datetime_request'].dt.day.map(lambda x: 1 if x <= 15 else 0)\n",
    "\n",
    "#replicate for dev data\n",
    "dev_feature = dev_data_df[[\n",
    "       'requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request',\n",
    "#        'requester_subreddits_at_request',\n",
    "       'requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request', 'month_request',\n",
    "       'week_request','weekday_request', 'hour_request']]\n",
    "\n",
    "dev_feature['length_of_request'] = dev_data_df['request_text_edit_aware'].map(lambda x: len(x))\n",
    "dev_feature['length_of_title'] = dev_data_df['request_title'].map(lambda x: len(x))\n",
    "dev_feature['first_half_of_month'] = dev_data_df['datetime_request'].dt.day.map(lambda x: 1 if x <= 15 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.575833\ttrain-auc:0.599788\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 20 rounds.\n",
      "[1]\teval-auc:0.578259\ttrain-auc:0.60058\n",
      "[2]\teval-auc:0.601538\ttrain-auc:0.634436\n",
      "[3]\teval-auc:0.601789\ttrain-auc:0.632173\n",
      "[4]\teval-auc:0.622508\ttrain-auc:0.655183\n",
      "[5]\teval-auc:0.622318\ttrain-auc:0.657012\n",
      "[6]\teval-auc:0.624965\ttrain-auc:0.67049\n",
      "[7]\teval-auc:0.632789\ttrain-auc:0.671292\n",
      "[8]\teval-auc:0.636024\ttrain-auc:0.667816\n",
      "[9]\teval-auc:0.638667\ttrain-auc:0.674926\n",
      "[10]\teval-auc:0.642188\ttrain-auc:0.67673\n",
      "[11]\teval-auc:0.649452\ttrain-auc:0.680999\n",
      "[12]\teval-auc:0.647221\ttrain-auc:0.684435\n",
      "[13]\teval-auc:0.655096\ttrain-auc:0.684353\n",
      "[14]\teval-auc:0.658018\ttrain-auc:0.69021\n",
      "[15]\teval-auc:0.655025\ttrain-auc:0.694001\n",
      "[16]\teval-auc:0.656664\ttrain-auc:0.696676\n",
      "[17]\teval-auc:0.659161\ttrain-auc:0.69946\n",
      "[18]\teval-auc:0.659923\ttrain-auc:0.702077\n",
      "[19]\teval-auc:0.660367\ttrain-auc:0.70189\n",
      "[20]\teval-auc:0.662038\ttrain-auc:0.703413\n",
      "[21]\teval-auc:0.661435\ttrain-auc:0.70568\n",
      "[22]\teval-auc:0.662479\ttrain-auc:0.707373\n",
      "[23]\teval-auc:0.665198\ttrain-auc:0.709126\n",
      "[24]\teval-auc:0.664888\ttrain-auc:0.709963\n",
      "[25]\teval-auc:0.664769\ttrain-auc:0.712451\n",
      "[26]\teval-auc:0.666079\ttrain-auc:0.713188\n",
      "[27]\teval-auc:0.667461\ttrain-auc:0.714633\n",
      "[28]\teval-auc:0.664277\ttrain-auc:0.71636\n",
      "[29]\teval-auc:0.663122\ttrain-auc:0.718149\n",
      "[30]\teval-auc:0.665353\ttrain-auc:0.717979\n",
      "[31]\teval-auc:0.666821\ttrain-auc:0.719241\n",
      "[32]\teval-auc:0.666508\ttrain-auc:0.721086\n",
      "[33]\teval-auc:0.666591\ttrain-auc:0.722819\n",
      "[34]\teval-auc:0.666984\ttrain-auc:0.723674\n",
      "[35]\teval-auc:0.667353\ttrain-auc:0.724436\n",
      "[36]\teval-auc:0.668032\ttrain-auc:0.726072\n",
      "[37]\teval-auc:0.668834\ttrain-auc:0.726876\n",
      "[38]\teval-auc:0.666921\ttrain-auc:0.728669\n",
      "[39]\teval-auc:0.666317\ttrain-auc:0.729698\n",
      "[40]\teval-auc:0.666528\ttrain-auc:0.730596\n",
      "[41]\teval-auc:0.666552\ttrain-auc:0.73164\n",
      "[42]\teval-auc:0.665436\ttrain-auc:0.733401\n",
      "[43]\teval-auc:0.664642\ttrain-auc:0.73353\n",
      "[44]\teval-auc:0.665329\ttrain-auc:0.73519\n",
      "[45]\teval-auc:0.664448\ttrain-auc:0.736514\n",
      "[46]\teval-auc:0.665944\ttrain-auc:0.736758\n",
      "[47]\teval-auc:0.666047\ttrain-auc:0.73884\n",
      "[48]\teval-auc:0.666623\ttrain-auc:0.740911\n",
      "[49]\teval-auc:0.667345\ttrain-auc:0.741833\n",
      "[50]\teval-auc:0.667028\ttrain-auc:0.741517\n",
      "[51]\teval-auc:0.667607\ttrain-auc:0.741899\n",
      "[52]\teval-auc:0.667552\ttrain-auc:0.743048\n",
      "[53]\teval-auc:0.666131\ttrain-auc:0.743653\n",
      "[54]\teval-auc:0.666246\ttrain-auc:0.744001\n",
      "[55]\teval-auc:0.665444\ttrain-auc:0.745186\n",
      "[56]\teval-auc:0.665214\ttrain-auc:0.746446\n",
      "[57]\teval-auc:0.66542\ttrain-auc:0.747182\n",
      "[58]\teval-auc:0.665845\ttrain-auc:0.74771\n",
      "[59]\teval-auc:0.665448\ttrain-auc:0.748312\n",
      "[60]\teval-auc:0.666865\ttrain-auc:0.749214\n",
      "[61]\teval-auc:0.666603\ttrain-auc:0.751631\n",
      "[62]\teval-auc:0.666619\ttrain-auc:0.752557\n",
      "[63]\teval-auc:0.666746\ttrain-auc:0.752904\n",
      "[64]\teval-auc:0.666222\ttrain-auc:0.753384\n",
      "[65]\teval-auc:0.666802\ttrain-auc:0.753991\n",
      "[66]\teval-auc:0.66673\ttrain-auc:0.754764\n",
      "[67]\teval-auc:0.66669\ttrain-auc:0.755475\n",
      "[68]\teval-auc:0.666643\ttrain-auc:0.755933\n",
      "[69]\teval-auc:0.666436\ttrain-auc:0.757398\n",
      "[70]\teval-auc:0.667052\ttrain-auc:0.758486\n",
      "[71]\teval-auc:0.666782\ttrain-auc:0.759784\n",
      "[72]\teval-auc:0.666139\ttrain-auc:0.760629\n",
      "[73]\teval-auc:0.66594\ttrain-auc:0.761623\n",
      "[74]\teval-auc:0.666194\ttrain-auc:0.761951\n",
      "[75]\teval-auc:0.666123\ttrain-auc:0.762942\n",
      "[76]\teval-auc:0.666448\ttrain-auc:0.763668\n",
      "[77]\teval-auc:0.666214\ttrain-auc:0.76408\n",
      "[78]\teval-auc:0.665452\ttrain-auc:0.764353\n",
      "[79]\teval-auc:0.66604\ttrain-auc:0.764923\n",
      "[80]\teval-auc:0.665746\ttrain-auc:0.765528\n",
      "[81]\teval-auc:0.667028\ttrain-auc:0.766761\n",
      "[82]\teval-auc:0.666948\ttrain-auc:0.767093\n",
      "[83]\teval-auc:0.666599\ttrain-auc:0.767651\n",
      "[84]\teval-auc:0.666317\ttrain-auc:0.768278\n",
      "[85]\teval-auc:0.665412\ttrain-auc:0.768574\n",
      "[86]\teval-auc:0.665079\ttrain-auc:0.769392\n",
      "[87]\teval-auc:0.665579\ttrain-auc:0.769967\n",
      "[88]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[89]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[90]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[91]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[92]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[93]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[94]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[95]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[96]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[97]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[98]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[99]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[100]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[101]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[102]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[103]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[104]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[105]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[106]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[107]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "[108]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "Stopping. Best iteration:\n",
      "[88]\teval-auc:0.666143\ttrain-auc:0.770099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# create request text features\n",
    "vectorizer1 = CountVectorizer(preprocessor = better_preprocessor) \n",
    "train_req_bag_of_words = vectorizer1.fit_transform(train_data_df.request_text_edit_aware)\n",
    "dev_req_bag_of_words = vectorizer1.transform(dev_data_df.request_text_edit_aware)\n",
    "\n",
    "# # create request text features\n",
    "# vectorizer2 = CountVectorizer(preprocessor = better_preprocessor) \n",
    "# train_title_bag_of_words = vectorizer2.fit_transform(train_data_df.request_title)\n",
    "# dev_title_bag_of_words = vectorizer2.transform(dev_data_df.request_title)\n",
    "\n",
    "# #create text features\n",
    "# vectorizer = TfidfVectorizer(preprocessor = better_preprocessor) \n",
    "# train_bag_of_words = vectorizer.fit_transform(train_data_df.request_text_edit_aware)\n",
    "# dev_bag_of_words = vectorizer.transform(dev_data_df.request_text_edit_aware)\n",
    "\n",
    "# train_bag_of_words.shape\n",
    "#create dmatrices\n",
    "dtrain = xgb.DMatrix(sparse.hstack((train_req_bag_of_words\n",
    "                           , train_feature\n",
    "                          )), train_labels)\n",
    "dtest = xgb.DMatrix(sparse.hstack((dev_req_bag_of_words\n",
    "                         , dev_feature\n",
    "                        )) , dev_labels)\n",
    "\n",
    "#booster parameter\n",
    "param = {'max_depth': 2, 'eta': 0.1, 'silent': 1, 'objective': 'binary:logistic'\n",
    "         , 'scale_pos_weight': 1, 'colsample_bytree': 1, 'subsample': 1, 'gamma': 5}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "#specify validation set to watch performance\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "#train model\n",
    "num_round = 500\n",
    "bst = xgb.train(param.items(), dtrain, num_round, evallist, early_stopping_rounds=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing using only date time features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n",
      "auc score using CountVectorizer & Logistic Regression = 0.4372454690514182\n"
     ]
    }
   ],
   "source": [
    "train_set = dt_dummies(train_data_df)\n",
    "\n",
    "dev_set = dt_dummies(dev_data_df)\n",
    "\n",
    "# create LR \n",
    "LR_1 = LogisticRegression()\n",
    "    \n",
    "# test the best value for C\n",
    "parameters = {'C': np.linspace(0.01, 10, 100)}\n",
    "\n",
    "# create GridSearchCV to find the best C\n",
    "clf = GridSearchCV(LR_1, parameters)\n",
    "    \n",
    "# # train the MultinomialNB\n",
    "clf.fit(train_set, train_labels)\n",
    "\n",
    "pred_dev_prob = clf.predict_proba(dev_set)[:,0]\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(\"auc score using CountVectorizer & Logistic Regression = {}\".format(roc_auc_score(dev_labels, pred_dev_prob, average='micro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LDA Topic Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanhe/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:497: RuntimeWarning: overflow encountered in exp\n",
      "  expElogthetad = np.exp(Elogthetad)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "texts = []\n",
    "\n",
    "for i in train_data_df.request_text_edit_aware[:200]:\n",
    "    \n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "#     stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "#     texts.append(stemmed_tokens)\n",
    "    texts.append(stopped_tokens)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (1,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (2,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (3,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (4,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (5,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (6,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (7,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (8,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"'),\n",
       " (9,\n",
       "  'nan*\"itll\" + nan*\"probably\" + nan*\"argue\" + nan*\"arrive\" + nan*\"breadsticks\" + nan*\"clock\" + nan*\"door\" + nan*\"addition\" + nan*\"o\" + nan*\"16\"')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics = 10, num_words = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
