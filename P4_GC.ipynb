{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurditchahal/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/gurditchahal/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC as LSVC\n",
    "from sklearn.decomposition import TruncatedSVD as TSVD\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# SK-learn libraries for model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# json libraries to parse json file\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4040, 32)\n",
      "Test shape:  (1631, 17)\n",
      "Columns in Train but not Test:\n",
      " {'requester_upvotes_minus_downvotes_at_retrieval', 'requester_account_age_in_days_at_retrieval', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_number_of_posts_at_retrieval', 'requester_received_pizza', 'post_was_edited', 'request_number_of_comments_at_retrieval', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_comments_at_retrieval', 'requester_user_flair', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_in_raop_at_retrieval', 'request_text', 'number_of_upvotes_of_request_at_retrieval', 'number_of_downvotes_of_request_at_retrieval'}\n",
      "\n",
      "Columns in Test but not Train: set()\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/gurditchahal/W207/coursework/Final_Project_Random_Acts/EDA\")\n",
    "# read json file\n",
    "train_json = json.load(open('train.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "train_json_df = json_normalize(train_json)\n",
    "\n",
    "# read json file\n",
    "test_json = json.load(open('test.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "test_json_df = json_normalize(test_json)\n",
    "\n",
    "print(\"Train shape: \", train_json_df.shape)\n",
    "print(\"Test shape: \", test_json_df.shape)\n",
    "\n",
    "train_only_columns = set(train_json_df.columns.values)-set(test_json_df.columns.values)\n",
    "print(\"Columns in Train but not Test:\\n\",train_only_columns)\n",
    "test_only_columns = set(test_json_df.columns.values)-set(train_json_df.columns.values)\n",
    "print(\"\\nColumns in Test but not Train:\",test_only_columns)\n",
    "test_w_train_col = train_json_df[test_json_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means the user doesn't receive pizza & 1 means the user receives pizza\n",
    "train_labels = train_json_df.requester_received_pizza.astype(int).as_matrix()\n",
    "\n",
    "# split the training data into training data and dev data \n",
    "train_data, dev_data, train_labels, dev_labels = \\\n",
    "            train_test_split(test_w_train_col, train_labels, test_size=0.2, random_state=12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=train_data['request_text_edit_aware'] +' '+train_data['request_title']\n",
    "dev_text=dev_data['request_text_edit_aware'] +' '+dev_data['request_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narratives per Standford paper\n",
    "money = [\"money\", \"now\", \"broke\", \"week\", \"until\", \"time\",\n",
    "          \"last\", \"day\", \"when\", \"today\", \"tonight\", \"paid\", \"next\",\n",
    "          \"first\", \"night\", \"after\", \"tomorrow\", \"month\", \"while\",\n",
    "          \"account\", \"before\", \"long\", \"Friday\", \"rent\", \"buy\",\n",
    "          \"bank\", \"still\", \"bills\", \"ago\", \"cash\", \"due\",\n",
    "          \"soon\", \"past\", \"never\", \"paycheck\", \"check\", \"spent\",\n",
    "          \"years\", \"poor\", \"till\", \"yesterday\", \"morning\", \"dollars\",\n",
    "          \"financial\", \"hour\", \"bill\", \"evening\", \"credit\",\n",
    "          \"budget\", \"loan\", \"bucks\", \"deposit\", \"dollar\", \"current\",\n",
    "          \"payed\"]\n",
    "job =[\"work\", \"job\", \"paycheck\", \"unemployment\", \"interview\",\n",
    "          \"fired\", \"employment\", \"hired\", \"hire\"]\n",
    "student = [\"college\", \"student\", \"school\", \"roommate\",\n",
    "          \"studying\", \"university\", \"finals\", \"semester\",\n",
    "          \"class\", \"study\", \"project\", \"dorm\", \"tuition\"]\n",
    "family =[\"family\", \"mom\", \"wife\", \"parents\", \"mother\", \"husband\",\n",
    "           \"dad\", \"son\", \"daughter\", \"father\", \"parent\",\n",
    "           \"mum\"]\n",
    "craving = [\"friend\", \"girlfriend\", \"craving\", \"birthday\",\n",
    "          \"boyfriend\", \"celebrate\", \"party\", \"game\", \"games\",\n",
    "          \"movie\", \"date\", \"drunk\", \"beer\", \"celebrating\", \"invited\",\n",
    "          \"drinks\", \"crave\", \"wasted\", \"invite\"]\n",
    "\n",
    "narratives = [money, job, student, family, craving]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def pre_proccess(s):\n",
    "    s = re.sub(\"[^\\w']|_\", \" \", s) \n",
    "    s=s.translate(str.maketrans(' ',' ',string.punctuation))#strip punctuation before looking\n",
    "    s=s.lower()\n",
    "   # p_stemmer = PorterStemmer()\n",
    "    #s = ' '.join([p_stemmer.stem(i) for i in s.split()])    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ft_mat(train_data):\n",
    "    feat_mat=pd.DataFrame()\n",
    "    feat_mat['hour_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.hour\n",
    "    feat_mat['hour_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.day\n",
    "    feat_mat['hour_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.month\n",
    "    #feat_mat['month_request']=pd.to_datetime(train_data['unix_timestamp_of_request'],unit = 's').dt.month\n",
    "    feat_mat['first_post']=np.log(train_data['requester_days_since_first_post_on_raop_at_request']+1)\n",
    "    feat_mat['upvotes_minus_downvotes']=train_data['requester_upvotes_minus_downvotes_at_request']\n",
    "    feat_mat['upvotes_plus_downvotes_at_request']=np.log(train_data['requester_upvotes_plus_downvotes_at_request']+1)\n",
    "    upvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request'] + row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    downvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request']- row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    feat_mat['upvotes']=upvotes\n",
    "    feat_mat['vote_ratio']=upvotes/(upvotes+downvotes+1)\n",
    "    feat_mat['req_age']=np.log(train_data['requester_account_age_in_days_at_request']+1)\n",
    "    feat_mat['num_subs']=np.log(train_data['requester_number_of_subreddits_at_request']+1)\n",
    "    feat_mat['num_posts']=np.log(train_data['requester_number_of_posts_at_request']+1)\n",
    "    feat_mat['pizza_activity']=np.log(train_data['requester_number_of_posts_on_raop_at_request']+1)\n",
    "    feat_mat['len_request']=np.log(train_data['request_text_edit_aware'].apply(len)+1)\n",
    "    feat_mat['len_title']=np.log(train_data['request_title'].apply(len)+1)\n",
    "    #feat_mat['len_name']=train_data['requester_username'].apply(len)\n",
    "    feat_mat['pizza_comments']=np.log(train_data['requester_number_of_comments_in_raop_at_request']+1)\n",
    "    feat_mat['hyperlink'] = train_data['request_text_edit_aware'].apply(lambda x: 1 if re.search(\"http|www\", x) else 0)\n",
    "    # reciprocity indicator\n",
    "    feat_mat['reciprocity'] = train_data['request_text_edit_aware'].apply(lambda x:1 if re.search(\"repay|pay.+back|pay.+forward|return.+favor\", x) \n",
    "                                               else 0)\n",
    "    feat_mat['politeness'] = train_data['request_text_edit_aware'].apply(lambda x: 1 if re.search(\"thank|appreciate|advance\", x) else 0)\n",
    "    \n",
    "    return feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_narr(narr,s):\n",
    "    ct=0\n",
    "    for word in narr:\n",
    "        ct+=s.split().count(word)\n",
    "    return ct/len(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat=construct_ft_mat(train_data)\n",
    "\n",
    "dev_mat=construct_ft_mat(dev_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_clean=train_text.apply(lambda s:pre_proccess(s))\n",
    "dev_text_clean=dev_text.apply(lambda s: pre_proccess(s))     \n",
    "for n in narratives:\n",
    "    feat_mat[n[0]]=train_text_clean.apply(lambda s: find_narr(n,s))\n",
    "    dev_mat[n[0]]=dev_text_clean.apply(lambda s: find_narr(n,s))\n",
    "    \n",
    "\n",
    "t_mat=feat_mat.as_matrix()\n",
    "\n",
    "d_mat=dev_mat.as_matrix()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,ngram_range=(1,2), preprocessor=pre_proccess,stop_words='english',norm='l2',sublinear_tf=True) \n",
    "train_bag_of_words = vectorizer.fit_transform(train_text)\n",
    "dev_bag_of_words = vectorizer.transform(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rf=RF()\\n#from sklearn.preprocessing import StandardScaler\\n#S=StandardScaler()\\n#clf=LogisticRegression()\\nr_parameters = {'n_estimators':[64,100,128],'criterion':['gini','entropy'],'random_state':[42],'max_depth':[None,1,3,5,7],'max_features':[None,'auto'],'min_samples_leaf':[1,3,5]}\\nclf=GridSearchCV(rf, r_parameters,scoring='roc_auc')\\nclf.fit(t_mat,train_labels)\\npreds=clf.predict(d_mat)\\nroc_auc_score(dev_labels, preds, average='micro')\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf=RF()\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#S=StandardScaler()\n",
    "#clf=LogisticRegression()\n",
    "r_parameters = {'n_estimators':[64,100,128],'criterion':['gini','entropy'],'random_state':[42],'max_depth':[None,1,3,5,7],'max_features':[None,'auto'],'min_samples_leaf':[1,3,5]}\n",
    "clf=GridSearchCV(rf, r_parameters,scoring='roc_auc')\n",
    "clf.fit(t_mat,train_labels)\n",
    "preds=clf.predict(d_mat)\n",
    "roc_auc_score(dev_labels, preds, average='micro')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232, 1037)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LSVC(C=1, penalty=\"l1\", dual=False,random_state=42).fit(train_bag_of_words,train_labels)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "#S=StandardScaler(with_mean=False)\n",
    "#X_new = model.transform(S.fit_transform(train_bag_of_words))\n",
    "X_new = model.transform(train_bag_of_words)\n",
    "print(X_new.shape)\n",
    "\n",
    "#d_new=model.transform(S.transform(dev_bag_of_words))\n",
    "d_new=model.transform(dev_bag_of_words)\n",
    "\n",
    "\n",
    "f_new=hstack([X_new,t_mat])\n",
    "dev_new=hstack([d_new,d_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'binarize': 1.0, 'alpha': 10.0}\n",
      "0.5942763157894737\n"
     ]
    }
   ],
   "source": [
    "nb=BernoulliNB()\n",
    "#nb=MultinomialNB()\n",
    "n_parameters = {'alpha': np.linspace(0.01, 10, 100),'binarize':np.linspace(0.0, 1, 10)}\n",
    "clf2 = GridSearchCV(nb, n_parameters,scoring='roc_auc',cv=5)\n",
    "clf2.fit(f_new,train_labels)\n",
    "preds=clf2.predict(dev_new)\n",
    "print(clf2.best_params_)\n",
    "print(roc_auc_score(dev_labels, preds, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5542434210526316"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.01, \n",
    "    max_depth=4, \n",
    "    random_state=123)\n",
    "\n",
    "GBC.fit(f_new,train_labels)\n",
    "preds=GBC.predict(dev_new)\n",
    "roc_auc_score(dev_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5311842105263158"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvm=LSVC(penalty='l2',dual=True)\n",
    "l_param={'C':np.linspace(0.001,1,100),'class_weight':['balanced',None]}\n",
    "csvm=GridSearchCV(lsvm,l_param,scoring='roc_auc',cv=5)\n",
    "S=StandardScaler(with_mean=False)\n",
    "csvm.fit(S.fit_transform(f_new),train_labels)\n",
    "preds=csvm.predict(S.transform(dev_new))\n",
    "roc_auc_score(dev_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.547447\ttrain-auc:0.747525\n",
      "[1]\teval-auc:0.6081\ttrain-auc:0.812512\n",
      "[2]\teval-auc:0.609248\ttrain-auc:0.844291\n",
      "[3]\teval-auc:0.606793\ttrain-auc:0.876057\n",
      "[4]\teval-auc:0.60428\ttrain-auc:0.893832\n",
      "[5]\teval-auc:0.601521\ttrain-auc:0.906241\n",
      "[6]\teval-auc:0.600197\ttrain-auc:0.91936\n",
      "[7]\teval-auc:0.603697\ttrain-auc:0.930068\n",
      "[8]\teval-auc:0.596669\ttrain-auc:0.943113\n",
      "[9]\teval-auc:0.609568\ttrain-auc:0.948715\n",
      "[10]\teval-auc:0.602829\ttrain-auc:0.956473\n",
      "[11]\teval-auc:0.604803\ttrain-auc:0.961777\n",
      "[12]\teval-auc:0.608113\ttrain-auc:0.967371\n",
      "[13]\teval-auc:0.613199\ttrain-auc:0.973009\n",
      "[14]\teval-auc:0.608939\ttrain-auc:0.979697\n",
      "[15]\teval-auc:0.609544\ttrain-auc:0.982361\n",
      "[16]\teval-auc:0.620872\ttrain-auc:0.986158\n",
      "[17]\teval-auc:0.621447\ttrain-auc:0.990306\n",
      "[18]\teval-auc:0.619178\ttrain-auc:0.992729\n",
      "[19]\teval-auc:0.624885\ttrain-auc:0.994274\n",
      "[20]\teval-auc:0.630502\ttrain-auc:0.995138\n",
      "[21]\teval-auc:0.6356\ttrain-auc:0.996798\n",
      "[22]\teval-auc:0.636209\ttrain-auc:0.997433\n",
      "[23]\teval-auc:0.63537\ttrain-auc:0.997915\n",
      "[24]\teval-auc:0.629992\ttrain-auc:0.998384\n",
      "[25]\teval-auc:0.629441\ttrain-auc:0.998811\n",
      "[26]\teval-auc:0.63278\ttrain-auc:0.99914\n",
      "[27]\teval-auc:0.639021\ttrain-auc:0.999168\n",
      "[28]\teval-auc:0.633775\ttrain-auc:0.999471\n",
      "[29]\teval-auc:0.637763\ttrain-auc:0.999565\n",
      "[30]\teval-auc:0.635247\ttrain-auc:0.999707\n",
      "[31]\teval-auc:0.637434\ttrain-auc:0.999803\n",
      "[32]\teval-auc:0.634868\ttrain-auc:0.999865\n",
      "[33]\teval-auc:0.631357\ttrain-auc:0.999879\n",
      "[34]\teval-auc:0.63384\ttrain-auc:0.999892\n",
      "[35]\teval-auc:0.633413\ttrain-auc:0.99983\n",
      "[36]\teval-auc:0.63338\ttrain-auc:0.999863\n",
      "[37]\teval-auc:0.635609\ttrain-auc:0.9999\n",
      "[38]\teval-auc:0.638873\ttrain-auc:0.999983\n",
      "[39]\teval-auc:0.636711\ttrain-auc:0.999986\n",
      "[40]\teval-auc:0.641192\ttrain-auc:0.999989\n",
      "[41]\teval-auc:0.645855\ttrain-auc:1\n",
      "[42]\teval-auc:0.64741\ttrain-auc:1\n",
      "[43]\teval-auc:0.646464\ttrain-auc:1\n",
      "[44]\teval-auc:0.649021\ttrain-auc:1\n",
      "[45]\teval-auc:0.650789\ttrain-auc:1\n",
      "[46]\teval-auc:0.648413\ttrain-auc:1\n",
      "[47]\teval-auc:0.650206\ttrain-auc:1\n",
      "[48]\teval-auc:0.648857\ttrain-auc:1\n",
      "[49]\teval-auc:0.65236\ttrain-auc:1\n",
      "[50]\teval-auc:0.644844\ttrain-auc:1\n",
      "[51]\teval-auc:0.643964\ttrain-auc:1\n",
      "[52]\teval-auc:0.649679\ttrain-auc:1\n",
      "[53]\teval-auc:0.649087\ttrain-auc:1\n",
      "[54]\teval-auc:0.649021\ttrain-auc:1\n",
      "[55]\teval-auc:0.65111\ttrain-auc:1\n",
      "[56]\teval-auc:0.652706\ttrain-auc:1\n",
      "[57]\teval-auc:0.650008\ttrain-auc:1\n",
      "[58]\teval-auc:0.650683\ttrain-auc:1\n",
      "[59]\teval-auc:0.654367\ttrain-auc:1\n",
      "[60]\teval-auc:0.656373\ttrain-auc:1\n",
      "[61]\teval-auc:0.6575\ttrain-auc:1\n",
      "[62]\teval-auc:0.656538\ttrain-auc:1\n",
      "[63]\teval-auc:0.658692\ttrain-auc:1\n",
      "[64]\teval-auc:0.658495\ttrain-auc:1\n",
      "[65]\teval-auc:0.657681\ttrain-auc:1\n",
      "[66]\teval-auc:0.65852\ttrain-auc:1\n",
      "[67]\teval-auc:0.657253\ttrain-auc:1\n",
      "[68]\teval-auc:0.659063\ttrain-auc:1\n",
      "[69]\teval-auc:0.659794\ttrain-auc:1\n",
      "[70]\teval-auc:0.660107\ttrain-auc:1\n",
      "[71]\teval-auc:0.660666\ttrain-auc:1\n",
      "[72]\teval-auc:0.658462\ttrain-auc:1\n",
      "[73]\teval-auc:0.659046\ttrain-auc:1\n",
      "[74]\teval-auc:0.660164\ttrain-auc:1\n",
      "[75]\teval-auc:0.660132\ttrain-auc:1\n",
      "[76]\teval-auc:0.660896\ttrain-auc:1\n",
      "[77]\teval-auc:0.66139\ttrain-auc:1\n",
      "[78]\teval-auc:0.661447\ttrain-auc:1\n",
      "[79]\teval-auc:0.660683\ttrain-auc:1\n",
      "[80]\teval-auc:0.66051\ttrain-auc:1\n",
      "[81]\teval-auc:0.659844\ttrain-auc:1\n",
      "[82]\teval-auc:0.662558\ttrain-auc:1\n",
      "[83]\teval-auc:0.661036\ttrain-auc:1\n",
      "[84]\teval-auc:0.661567\ttrain-auc:1\n",
      "[85]\teval-auc:0.662097\ttrain-auc:1\n",
      "[86]\teval-auc:0.659572\ttrain-auc:1\n",
      "[87]\teval-auc:0.660296\ttrain-auc:1\n",
      "[88]\teval-auc:0.660699\ttrain-auc:1\n",
      "[89]\teval-auc:0.661785\ttrain-auc:1\n",
      "[90]\teval-auc:0.662385\ttrain-auc:1\n",
      "[91]\teval-auc:0.664079\ttrain-auc:1\n",
      "[92]\teval-auc:0.664507\ttrain-auc:1\n",
      "[93]\teval-auc:0.66389\ttrain-auc:1\n",
      "[94]\teval-auc:0.665074\ttrain-auc:1\n",
      "[95]\teval-auc:0.662261\ttrain-auc:1\n",
      "[96]\teval-auc:0.663586\ttrain-auc:1\n",
      "[97]\teval-auc:0.66384\ttrain-auc:1\n",
      "[98]\teval-auc:0.662401\ttrain-auc:1\n",
      "[99]\teval-auc:0.663446\ttrain-auc:1\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "#create dmatrices\n",
    "dtrain = xgb.DMatrix(f_new, train_labels)\n",
    "dtest = xgb.DMatrix(dev_new\n",
    "                         , dev_labels)\n",
    "\n",
    "#booster parameter\n",
    "param = {'max_depth': 7, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'\n",
    "         , 'scale_pos_weight': 3.06,'max_delta_step':1,'subsample':.8}#9 depth if sublin false\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "#specify validation set to watch performance\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "#train model\n",
    "num_round = 100\n",
    "bst = xgb.train(param.items(), dtrain, num_round, evallist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
