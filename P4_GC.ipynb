{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC as LSVC\n",
    "from sklearn.decomposition import TruncatedSVD as TSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# SK-learn libraries for model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# json libraries to parse json file\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4040, 32)\n",
      "Test shape:  (1631, 17)\n",
      "Columns in Train but not Test:\n",
      " {'requester_upvotes_plus_downvotes_at_retrieval', 'post_was_edited', 'number_of_upvotes_of_request_at_retrieval', 'requester_number_of_posts_on_raop_at_retrieval', 'request_text', 'request_number_of_comments_at_retrieval', 'requester_received_pizza', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_user_flair', 'requester_number_of_comments_at_retrieval', 'requester_number_of_posts_at_retrieval', 'number_of_downvotes_of_request_at_retrieval', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_retrieval'}\n",
      "\n",
      "Columns in Test but not Train: set()\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/gurditchahal/W207/coursework/Final_Project_Random_Acts/EDA\")\n",
    "# read json file\n",
    "train_json = json.load(open('train.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "train_json_df = json_normalize(train_json)\n",
    "\n",
    "# read json file\n",
    "test_json = json.load(open('test.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "test_json_df = json_normalize(test_json)\n",
    "\n",
    "print(\"Train shape: \", train_json_df.shape)\n",
    "print(\"Test shape: \", test_json_df.shape)\n",
    "\n",
    "train_only_columns = set(train_json_df.columns.values)-set(test_json_df.columns.values)\n",
    "print(\"Columns in Train but not Test:\\n\",train_only_columns)\n",
    "test_only_columns = set(test_json_df.columns.values)-set(train_json_df.columns.values)\n",
    "print(\"\\nColumns in Test but not Train:\",test_only_columns)\n",
    "test_w_train_col = train_json_df[test_json_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known',\n",
       " 'request_id',\n",
       " 'request_text_edit_aware',\n",
       " 'request_title',\n",
       " 'requester_account_age_in_days_at_request',\n",
       " 'requester_days_since_first_post_on_raop_at_request',\n",
       " 'requester_number_of_comments_at_request',\n",
       " 'requester_number_of_comments_in_raop_at_request',\n",
       " 'requester_number_of_posts_at_request',\n",
       " 'requester_number_of_posts_on_raop_at_request',\n",
       " 'requester_number_of_subreddits_at_request',\n",
       " 'requester_subreddits_at_request',\n",
       " 'requester_upvotes_minus_downvotes_at_request',\n",
       " 'requester_upvotes_plus_downvotes_at_request',\n",
       " 'requester_username',\n",
       " 'unix_timestamp_of_request',\n",
       " 'unix_timestamp_of_request_utc'}"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_json_df.columns.values).intersection(set(test_json_df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means the user doesn't receive pizza & 1 means the user receives pizza\n",
    "train_labels = train_json_df.requester_received_pizza.astype(int).as_matrix()\n",
    "previous_givers=set(train_json_df[\"giver_username_if_known\"])\n",
    "previous_givers.remove('N/A')\n",
    "# split the training data into training data and dev data \n",
    "train_data, dev_data, train_labels, dev_labels = \\\n",
    "            train_test_split(test_w_train_col, train_labels, test_size=0.2, random_state=12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurditchahal/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/gurditchahal/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_data['full_text']=train_data['request_text_edit_aware'] +' '+train_data['request_title']\n",
    "dev_data['full_text']=dev_data['request_text_edit_aware'] +' '+dev_data['request_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narratives per Standford paper\n",
    "money = [\"money\", \"now\", \"broke\", \"week\", \"until\", \"time\",\n",
    "          \"last\", \"day\", \"when\", \"today\", \"tonight\", \"paid\", \"next\",\n",
    "          \"first\", \"night\", \"after\", \"tomorrow\", \"month\", \"while\",\n",
    "          \"account\", \"before\", \"long\", \"Friday\", \"rent\", \"buy\",\n",
    "          \"bank\", \"still\", \"bills\", \"ago\", \"cash\", \"due\",\n",
    "          \"soon\", \"past\", \"never\", \"paycheck\", \"check\", \"spent\",\n",
    "          \"years\", \"poor\", \"till\", \"yesterday\", \"morning\", \"dollars\",\n",
    "          \"financial\", \"hour\", \"bill\", \"evening\", \"credit\",\n",
    "          \"budget\", \"loan\", \"bucks\", \"deposit\", \"dollar\", \"current\",\n",
    "          \"payed\"]\n",
    "job =[\"work\", \"job\", \"paycheck\", \"unemployment\", \"interview\",\n",
    "          \"fired\", \"employment\", \"hired\", \"hire\"]\n",
    "student = [\"college\", \"student\", \"school\", \"roommate\",\n",
    "          \"studying\", \"university\", \"finals\", \"semester\",\n",
    "          \"class\", \"study\", \"project\", \"dorm\", \"tuition\"]\n",
    "family =[\"family\", \"mom\", \"wife\", \"parents\", \"mother\", \"husband\",\n",
    "           \"dad\", \"son\", \"daughter\", \"father\", \"parent\",\n",
    "           \"mum\"]\n",
    "craving = [\"friend\", \"girlfriend\", \"craving\", \"birthday\",\n",
    "          \"boyfriend\", \"celebrate\", \"party\", \"game\", \"games\",\n",
    "          \"movie\", \"date\", \"drunk\", \"beer\", \"celebrating\", \"invited\",\n",
    "          \"drinks\", \"crave\", \"wasted\", \"invite\"]\n",
    "\n",
    "narratives = [money, job, student, family, craving]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def pre_proccess(s):\n",
    "    s = re.sub(\"[^\\w']|_\", \" \", s) \n",
    "    s=s.translate(str.maketrans(' ',' ',string.punctuation))#strip punctuation before looking\n",
    "    s= re.sub(' +',' ', s) #remove extra spaces\n",
    "    s=s.lower()\n",
    "   # p_stemmer = PorterStemmer()\n",
    "    #s = ' '.join([p_stemmer.stem(i) for i in s.split()])    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.timeanddate.com/calendar/aboutseasons.html\n",
    "def ts_to_season(month):\n",
    "    if month>=3 and month<=5:\n",
    "        return \"spring\"\n",
    "    elif month>=6 and month <=8:\n",
    "        return \"summer\"\n",
    "    elif month>=9 and month <=11:\n",
    "        return \"fall\"\n",
    "    else:\n",
    "        return \"winter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ft_mat(train_data):\n",
    "    feat_mat=pd.DataFrame()\n",
    "    \n",
    "    #temporal features\n",
    "    feat_mat['hour_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.hour\n",
    "    feat_mat['day_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.day\n",
    "    feat_mat['day_request']=feat_mat['day_request'].apply(lambda x: 0 if x<16 else 1)\n",
    "    feat_mat['season_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.month\n",
    "    feat_mat['season_request']=feat_mat['season_request'].apply(ts_to_season)\n",
    "    feat_mat['is_spring']=feat_mat['season_request'].apply(lambda x: 1 if x=='spring' else 0)\n",
    "    feat_mat['is_summer']=feat_mat['season_request'].apply(lambda x: 1 if x=='summer' else 0)\n",
    "    feat_mat['is_fall']=feat_mat['season_request'].apply(lambda x: 1 if x=='fall' else 0)\n",
    "    feat_mat['is_winter']=feat_mat['season_request'].apply(lambda x: 1 if x=='winter' else 0)\n",
    "    del feat_mat['season_request']\n",
    "    \n",
    "    #check if requester was a previous giver\n",
    "    #feat_mat['was_giver']=train_data['requester_username'].apply(lambda x: 1 if x in previous_givers else 0)\n",
    "    \n",
    "    feat_mat['first_post']=np.log(train_data['requester_days_since_first_post_on_raop_at_request']+1)\n",
    "    feat_mat['upvotes_minus_downvotes']=train_data['requester_upvotes_minus_downvotes_at_request']\n",
    "    feat_mat['upvotes_plus_downvotes_at_request']=np.log(train_data['requester_upvotes_plus_downvotes_at_request']+1)\n",
    "    upvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request'] + row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    downvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request']- row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    feat_mat['upvotes']=upvotes\n",
    "    feat_mat['vote_ratio']=upvotes/(upvotes+downvotes+1)\n",
    "    \n",
    "    feat_mat['req_age']=np.log(train_data['requester_account_age_in_days_at_request']+1)\n",
    "    feat_mat['num_subs']=np.log(train_data['requester_number_of_subreddits_at_request']+1)\n",
    "    feat_mat['num_posts']=np.log(train_data['requester_number_of_posts_at_request']+1)\n",
    "    feat_mat['pizza_activity']=np.log(train_data['requester_number_of_posts_on_raop_at_request']+1)\n",
    "    feat_mat['len_request']=np.log(train_data['request_text_edit_aware'].apply(len)+1)\n",
    "    feat_mat['len_title']=np.log(train_data['request_title'].apply(len)+1)\n",
    "    #feat_mat['len_name']=train_data['requester_username'].apply(len)\n",
    "    feat_mat['pizza_comments']=np.log(train_data['requester_number_of_comments_in_raop_at_request']+1)\n",
    "    # reciprocity indicator\n",
    "    #feat_mat['reciprocity'] = train_data['request_text_edit_aware'].apply(lambda x:1 if re.search(\"repay|pay.+back|pay.+forward|return.+favor\", x) \n",
    "                                               #else 0)\n",
    "    #feat_mat['image_in_text'] = train_data['request_text_edit_aware'].str.contains('imgur.com|.jpg|.png|.jpeg', case=False).apply(lambda x: 1 if x else 0)\n",
    "    #feat_mat['politeness'] = train_data['request_text_edit_aware'].apply(lambda x: 1 if re.search(\"thank|appreciate|advance\", x) else 0)\n",
    "    feat_mat['reciprocity'] = train_data['full_text'].apply(lambda x:1 if re.search(\"repay|pay.+back|pay.+forward|return.+favor\", x) \n",
    "                                               else 0)\n",
    "    feat_mat['image_in_text'] = train_data['full_text'].str.contains('imgur.com|.jpg|.png|.jpeg', case=False).apply(lambda x: 1 if x else 0)\n",
    "    feat_mat['politeness'] = train_data['full_text'].apply(lambda x: 1 if re.search(\"thank|appreciate|advance\", x) else 0)\n",
    "    \n",
    "    return feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_narr(narr,s):\n",
    "    ct=0\n",
    "    for word in narr:\n",
    "        ct+=s.split().count(word)\n",
    "    return ct/len(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat=construct_ft_mat(train_data)\n",
    "\n",
    "dev_mat=construct_ft_mat(dev_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_clean=train_data['full_text'].apply(lambda s:pre_proccess(s))\n",
    "dev_text_clean=dev_data['full_text'].apply(lambda s: pre_proccess(s))     \n",
    "for n in narratives:\n",
    "    feat_mat[n[0]]=train_text_clean.apply(lambda s: find_narr(n,s))\n",
    "    dev_mat[n[0]]=dev_text_clean.apply(lambda s: find_narr(n,s))\n",
    "    \n",
    "\n",
    "t_mat=feat_mat.as_matrix()\n",
    "\n",
    "d_mat=dev_mat.as_matrix()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,ngram_range=(1,2), preprocessor=pre_proccess,stop_words='english',norm='l2',sublinear_tf=True) \n",
    "train_bag_of_words = vectorizer.fit_transform(train_data['full_text'])\n",
    "dev_bag_of_words = vectorizer.transform(dev_data['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rf=RF()\\n#from sklearn.preprocessing import StandardScaler\\n#S=StandardScaler()\\n#clf=LogisticRegression()\\nr_parameters = {'n_estimators':[64,100,128],'criterion':['gini','entropy'],'random_state':[42],'max_depth':[None,1,3,5,7],'max_features':[None,'auto'],'min_samples_leaf':[1,3,5]}\\nclf=GridSearchCV(rf, r_parameters,scoring='roc_auc')\\nclf.fit(t_mat,train_labels)\\npreds=clf.predict(d_mat)\\nroc_auc_score(dev_labels, preds, average='micro')\""
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf=RF()\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#S=StandardScaler()\n",
    "#clf=LogisticRegression()\n",
    "r_parameters = {'n_estimators':[64,100,128],'criterion':['gini','entropy'],'random_state':[42],'max_depth':[None,1,3,5,7],'max_features':[None,'auto'],'min_samples_leaf':[1,3,5]}\n",
    "clf=GridSearchCV(rf, r_parameters,scoring='roc_auc')\n",
    "clf.fit(t_mat,train_labels)\n",
    "preds=clf.predict(d_mat)\n",
    "roc_auc_score(dev_labels, preds, average='micro')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232, 873)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LSVC(C=.85, penalty=\"l1\", dual=False,random_state=42).fit(train_bag_of_words,train_labels)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "#S=StandardScaler(with_mean=False)\n",
    "#X_new = model.transform(S.fit_transform(train_bag_of_words))\n",
    "X_new = model.transform(train_bag_of_words)\n",
    "print(X_new.shape)\n",
    "\n",
    "#d_new=model.transform(S.transform(dev_bag_of_words))\n",
    "d_new=model.transform(dev_bag_of_words)\n",
    "\n",
    "\n",
    "f_new=hstack([X_new,t_mat])\n",
    "dev_new=hstack([d_new,d_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_lda = CountVectorizer(min_df=10,ngram_range=(1,1), preprocessor=pre_proccess,stop_words='english') \n",
    "lda_bag_of_words = vectorizer_lda.fit_transform(train_data['full_text'])\n",
    "lda_devbag_of_words = vectorizer_lda.transform(dev_data['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1580)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "lda = LDA(n_components=3, learning_method=\"batch\", max_iter=30,learning_decay=.7, random_state=42)\n",
    "train_topics = lda.fit_transform(lda_bag_of_words)\n",
    "print(lda.components_.shape)\n",
    "\n",
    "dev_topics=lda.transform(lda_devbag_of_words)\n",
    "\n",
    "\n",
    "f_new=np.hstack([t_mat,train_topics])\n",
    "dev_new=np.hstack([d_mat,dev_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new=hstack([X_new,t_mat,train_topics])\n",
    "dev_new=hstack([d_new,d_mat,dev_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pca=PCA(n_components=5,random_state=42)\n",
    "word_compresst=pca.fit_transform(t_mat)\n",
    "word_compresstest=pca.transform(d_mat)\n",
    "\n",
    "f_new=hstack([X_new,t_mat])\n",
    "dev_new=hstack([d_new,d_mat])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nb=BernoulliNB()\\n#nb=MultinomialNB()\\nn_parameters = {'alpha': np.linspace(0.01, 10, 100),'binarize':np.linspace(0.0, 1, 10)}\\nclf2 = GridSearchCV(nb, n_parameters,scoring='roc_auc',cv=5)\\nclf2.fit(f_new,train_labels)\\npreds=clf2.predict(dev_new)\\nprint(clf2.best_params_)\\nprint(roc_auc_score(dev_labels, preds, average='micro'))\""
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nb=BernoulliNB()\n",
    "#nb=MultinomialNB()\n",
    "n_parameters = {'alpha': np.linspace(0.01, 10, 100),'binarize':np.linspace(0.0, 1, 10)}\n",
    "clf2 = GridSearchCV(nb, n_parameters,scoring='roc_auc',cv=5)\n",
    "clf2.fit(f_new,train_labels)\n",
    "preds=clf2.predict(dev_new)\n",
    "print(clf2.best_params_)\n",
    "print(roc_auc_score(dev_labels, preds, average='micro'))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import GradientBoostingClassifier\\nGBC = GradientBoostingClassifier(\\n    n_estimators=500, \\n    learning_rate=0.01, \\n    max_depth=4, \\n    random_state=123)\\n\\nGBC.fit(f_new,train_labels)\\npreds=GBC.predict(dev_new)\\nroc_auc_score(dev_labels, preds, average='micro')\""
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.01, \n",
    "    max_depth=4, \n",
    "    random_state=123)\n",
    "\n",
    "GBC.fit(f_new,train_labels)\n",
    "preds=GBC.predict(dev_new)\n",
    "roc_auc_score(dev_labels, preds, average='micro')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lsvm=LSVC(penalty='l2',dual=True)\\nl_param={'C':np.linspace(0.001,1,100),'class_weight':['balanced',None]}\\ncsvm=GridSearchCV(lsvm,l_param,scoring='roc_auc',cv=5)\\nS=StandardScaler(with_mean=False)\\ncsvm.fit(S.fit_transform(f_new),train_labels)\\npreds=csvm.predict(S.transform(dev_new))\\nroc_auc_score(dev_labels, preds, average='micro')\""
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''lsvm=LSVC(penalty='l2',dual=True)\n",
    "l_param={'C':np.linspace(0.001,1,100),'class_weight':['balanced',None]}\n",
    "csvm=GridSearchCV(lsvm,l_param,scoring='roc_auc',cv=5)\n",
    "S=StandardScaler(with_mean=False)\n",
    "csvm.fit(S.fit_transform(f_new),train_labels)\n",
    "preds=csvm.predict(S.transform(dev_new))\n",
    "roc_auc_score(dev_labels, preds, average='micro')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.547878\ttrain-auc:0.83709\n",
      "[1]\teval-auc:0.602936\ttrain-auc:0.930538\n",
      "[2]\teval-auc:0.62236\ttrain-auc:0.952502\n",
      "[3]\teval-auc:0.621624\ttrain-auc:0.955419\n",
      "[4]\teval-auc:0.625415\ttrain-auc:0.957615\n",
      "[5]\teval-auc:0.628228\ttrain-auc:0.957443\n",
      "[6]\teval-auc:0.625498\ttrain-auc:0.960893\n",
      "[7]\teval-auc:0.631743\ttrain-auc:0.965212\n",
      "[8]\teval-auc:0.630839\ttrain-auc:0.974724\n",
      "[9]\teval-auc:0.636509\ttrain-auc:0.975798\n",
      "[10]\teval-auc:0.637229\ttrain-auc:0.978529\n",
      "[11]\teval-auc:0.640691\ttrain-auc:0.982643\n",
      "[12]\teval-auc:0.643705\ttrain-auc:0.983878\n",
      "[13]\teval-auc:0.642286\ttrain-auc:0.984687\n",
      "[14]\teval-auc:0.647578\ttrain-auc:0.985296\n",
      "[15]\teval-auc:0.645646\ttrain-auc:0.985778\n",
      "[16]\teval-auc:0.649005\ttrain-auc:0.987255\n",
      "[17]\teval-auc:0.651447\ttrain-auc:0.986858\n",
      "[18]\teval-auc:0.656924\ttrain-auc:0.988852\n",
      "[19]\teval-auc:0.655896\ttrain-auc:0.988894\n",
      "[20]\teval-auc:0.655699\ttrain-auc:0.98926\n",
      "[21]\teval-auc:0.654856\ttrain-auc:0.989358\n",
      "[22]\teval-auc:0.654947\ttrain-auc:0.990026\n",
      "[23]\teval-auc:0.655666\ttrain-auc:0.990416\n",
      "[24]\teval-auc:0.657961\ttrain-auc:0.991511\n",
      "[25]\teval-auc:0.655892\ttrain-auc:0.991983\n",
      "[26]\teval-auc:0.654803\ttrain-auc:0.991927\n",
      "[27]\teval-auc:0.656188\ttrain-auc:0.992901\n",
      "[28]\teval-auc:0.656571\ttrain-auc:0.993193\n",
      "[29]\teval-auc:0.656801\ttrain-auc:0.993736\n",
      "[30]\teval-auc:0.661546\ttrain-auc:0.994603\n",
      "[31]\teval-auc:0.661024\ttrain-auc:0.994918\n",
      "[32]\teval-auc:0.661698\ttrain-auc:0.994905\n",
      "[33]\teval-auc:0.660366\ttrain-auc:0.994905\n",
      "[34]\teval-auc:0.660354\ttrain-auc:0.99521\n",
      "[35]\teval-auc:0.663059\ttrain-auc:0.995633\n",
      "[36]\teval-auc:0.66347\ttrain-auc:0.99596\n",
      "[37]\teval-auc:0.663232\ttrain-auc:0.996325\n",
      "[38]\teval-auc:0.66426\ttrain-auc:0.996772\n",
      "[39]\teval-auc:0.663956\ttrain-auc:0.996958\n",
      "[40]\teval-auc:0.666587\ttrain-auc:0.996944\n",
      "[41]\teval-auc:0.666653\ttrain-auc:0.997241\n",
      "[42]\teval-auc:0.668487\ttrain-auc:0.997517\n",
      "[43]\teval-auc:0.669104\ttrain-auc:0.997533\n",
      "[44]\teval-auc:0.669375\ttrain-auc:0.997707\n",
      "[45]\teval-auc:0.668487\ttrain-auc:0.997775\n",
      "[46]\teval-auc:0.670041\ttrain-auc:0.997894\n",
      "[47]\teval-auc:0.669556\ttrain-auc:0.997981\n",
      "[48]\teval-auc:0.670929\ttrain-auc:0.998114\n",
      "[49]\teval-auc:0.671151\ttrain-auc:0.9982\n",
      "[50]\teval-auc:0.67162\ttrain-auc:0.99823\n",
      "[51]\teval-auc:0.674819\ttrain-auc:0.998292\n",
      "[52]\teval-auc:0.675502\ttrain-auc:0.99844\n",
      "[53]\teval-auc:0.675658\ttrain-auc:0.998556\n",
      "[54]\teval-auc:0.675715\ttrain-auc:0.998497\n",
      "[55]\teval-auc:0.676628\ttrain-auc:0.998681\n",
      "[56]\teval-auc:0.677845\ttrain-auc:0.998703\n",
      "[57]\teval-auc:0.678931\ttrain-auc:0.998784\n",
      "[58]\teval-auc:0.680189\ttrain-auc:0.998826\n",
      "[59]\teval-auc:0.679383\ttrain-auc:0.998876\n",
      "[60]\teval-auc:0.680041\ttrain-auc:0.998878\n",
      "[61]\teval-auc:0.68009\ttrain-auc:0.998952\n",
      "[62]\teval-auc:0.679778\ttrain-auc:0.999071\n",
      "[63]\teval-auc:0.680872\ttrain-auc:0.999091\n",
      "[64]\teval-auc:0.682422\ttrain-auc:0.999149\n",
      "[65]\teval-auc:0.682681\ttrain-auc:0.999167\n",
      "[66]\teval-auc:0.682525\ttrain-auc:0.999204\n",
      "[67]\teval-auc:0.684169\ttrain-auc:0.999265\n",
      "[68]\teval-auc:0.683121\ttrain-auc:0.999349\n",
      "[69]\teval-auc:0.683503\ttrain-auc:0.999362\n",
      "[70]\teval-auc:0.684367\ttrain-auc:0.999386\n",
      "[71]\teval-auc:0.68509\ttrain-auc:0.999439\n",
      "[72]\teval-auc:0.685814\ttrain-auc:0.999483\n",
      "[73]\teval-auc:0.687097\ttrain-auc:0.999498\n",
      "[74]\teval-auc:0.688232\ttrain-auc:0.999505\n",
      "[75]\teval-auc:0.68912\ttrain-auc:0.999543\n",
      "[76]\teval-auc:0.688684\ttrain-auc:0.999601\n",
      "[77]\teval-auc:0.689211\ttrain-auc:0.999614\n",
      "[78]\teval-auc:0.689893\ttrain-auc:0.999653\n",
      "[79]\teval-auc:0.68963\ttrain-auc:0.999667\n",
      "[80]\teval-auc:0.690461\ttrain-auc:0.999684\n",
      "[81]\teval-auc:0.690707\ttrain-auc:0.999711\n",
      "[82]\teval-auc:0.691414\ttrain-auc:0.999737\n",
      "[83]\teval-auc:0.692442\ttrain-auc:0.999753\n",
      "[84]\teval-auc:0.691949\ttrain-auc:0.999755\n",
      "[85]\teval-auc:0.692311\ttrain-auc:0.999761\n",
      "[86]\teval-auc:0.69347\ttrain-auc:0.999774\n",
      "[87]\teval-auc:0.694005\ttrain-auc:0.999785\n",
      "[88]\teval-auc:0.693273\ttrain-auc:0.999797\n",
      "[89]\teval-auc:0.692895\ttrain-auc:0.999799\n",
      "[90]\teval-auc:0.692015\ttrain-auc:0.999807\n",
      "[91]\teval-auc:0.692722\ttrain-auc:0.99982\n",
      "[92]\teval-auc:0.692919\ttrain-auc:0.999827\n",
      "[93]\teval-auc:0.692738\ttrain-auc:0.99984\n",
      "[94]\teval-auc:0.691867\ttrain-auc:0.999849\n",
      "[95]\teval-auc:0.691974\ttrain-auc:0.999851\n",
      "[96]\teval-auc:0.69259\ttrain-auc:0.999854\n",
      "[97]\teval-auc:0.692936\ttrain-auc:0.999858\n",
      "[98]\teval-auc:0.692796\ttrain-auc:0.999874\n",
      "[99]\teval-auc:0.693289\ttrain-auc:0.999888\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "#create dmatrices\n",
    "dtrain = xgb.DMatrix(f_new, train_labels)\n",
    "dtest = xgb.DMatrix(dev_new\n",
    "                         , dev_labels)\n",
    "\n",
    "#booster parameter\n",
    "param = {'max_depth':15, 'eta': .015, 'silent': 1, 'objective': 'binary:logistic'\n",
    "         , 'scale_pos_weight': 3.06,'max_delta_step':1,'subsample':.9,'seed':42}#9 depth if sublin false\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "#specify validation set to watch performance\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "#train model\n",
    "num_round = 100\n",
    "bst = xgb.train(param.items(), dtrain, num_round, evallist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function pre_proccess at 0x10efe5620>,\n",
       "        stop_words='english', strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat=construct_ft_mat(test_json_df)\n",
    "test_mat['full_text']=test_data['request_text_edit_aware'] +' '+test_data['request_title']\n",
    "test_text_clean=test_data['full_text'].apply(lambda s: pre_proccess(s))\n",
    "for n in narratives:\n",
    "    test_mat[n[0]]=test_text_clean.apply(lambda s: find_narr(n,s))\n",
    "    \n",
    "lda_testbag_of_words = vectorizer_lda.transform(test_data['full_text'])    \n",
    "test_labels = test_json_df.requester_received_pizza.astype(int).as_matrix()\n",
    "\n",
    "test_bag_of_words = vectorizer.fit_transform(test_data['full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
