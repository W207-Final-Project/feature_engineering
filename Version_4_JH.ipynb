{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Final Project - Random Acts of Pizza\n",
    "### Predicting altruism through free pizza\n",
    "\n",
    "#### Team Members: Gurdit Chahal, Shan He, Joanna Huang,  Emmy Lau\n",
    "\n",
    "This project is originated from the Kaggle competition https://www.kaggle.com/c/random-acts-of-pizza. We will create an algorithm to predict which requests will recieve pizza and which on will not.  The competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. This data was collected and graciously shared by Althoff et al (http://www.timalthoff.com/). \n",
    "\n",
    "**Reference Paper:**\n",
    "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014. (http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize packages and load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC as LSVC\n",
    "from sklearn.decomposition import TruncatedSVD as TSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# SK-learn libraries for model selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# json libraries to parse json file\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import lda\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4040, 32)\n",
      "Test shape:  (1631, 17)\n"
     ]
    }
   ],
   "source": [
    "# read json file\n",
    "train_json = json.load(open('train.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "train_json_df = json_normalize(train_json)\n",
    "\n",
    "# read json file\n",
    "test_json = json.load(open('test.json'))\n",
    "\n",
    "# normalize data and put in a dataframe\n",
    "test_json_df = json_normalize(test_json)\n",
    "\n",
    "print(\"Train shape: \", train_json_df.shape)\n",
    "print(\"Test shape: \", test_json_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a discrepancy between the train and test datasets shapes, with the training set having 32 columns and the test set only having 17. Let's take a closer look at the significance of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Train but not Test:\n",
      " {'requester_number_of_posts_at_retrieval', 'post_was_edited', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_upvotes_minus_downvotes_at_retrieval', 'request_text', 'requester_received_pizza', 'request_number_of_comments_at_retrieval', 'requester_days_since_first_post_on_raop_at_retrieval', 'number_of_downvotes_of_request_at_retrieval', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_number_of_comments_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'requester_account_age_in_days_at_retrieval'}\n",
      "\n",
      "Columns in Test but not Train: set()\n"
     ]
    }
   ],
   "source": [
    "train_only_columns = set(train_json_df.columns.values)-set(test_json_df.columns.values)\n",
    "print(\"Columns in Train but not Test:\\n\",train_only_columns)\n",
    "test_only_columns = set(test_json_df.columns.values)-set(train_json_df.columns.values)\n",
    "print(\"\\nColumns in Test but not Train:\",test_only_columns)\n",
    "test_w_train_col = train_json_df[test_json_df.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details on the additional columns in the train set:\n",
    "\n",
    "* request_text/post_was_edited: Since posts are often edited after a successful request, this request_text column is not the most accurate. Instead, request_text_edit_aware, which is available in both the train and test sets, will be used. This edit aware version of \"request_text\" strips edited comments indicating the success of the request.\n",
    "\n",
    "* *_at_retrieval: For our purposes of real-time prediction, _at_request columns are more relevant.\n",
    "\n",
    "* requester_user_flair: This is a post-receipt of pizza feature and thus will not be a useful indicator of results.\n",
    "\n",
    "* requester_received_pizza: To be predicted\n",
    "\n",
    "We will move forward with only the columns in both the train and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train and dev for model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 means the user doesn't receive pizza & 1 means the user receives pizza\n",
    "train_labels = train_json_df.requester_received_pizza.astype(int).as_matrix()\n",
    "\n",
    "# split the training data into training data and dev data \n",
    "train_data, dev_data, train_labels, dev_labels = \\\n",
    "            train_test_split(test_w_train_col, train_labels, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by prepping our text data, creating a new column \"full text\" that combines all the relevant text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_data['full_text']=train_data['request_text_edit_aware'] +' '+train_data['request_title']\n",
    "dev_data['full_text']=dev_data['request_text_edit_aware'] +' '+dev_data['request_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation of further processing, we will preprocess our text in the following ways: converting all text to lowercase, removing punctuation, non-alphanumeric characters and extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_proccess(s):\n",
    "    s = re.sub(\"[^\\w']|_\", \" \", s) \n",
    "    s=s.translate(str.maketrans(' ',' ',string.punctuation))# Strip punctuation before looking\n",
    "    s= re.sub(' +',' ', s) # Remove extra spaces\n",
    "    s=s.lower()\n",
    "    #s = ' '.join(word[:6] if len(word)>6 else word \n",
    "                        # for word in s.split()) # Shorten long words\n",
    "    #s = re.sub(r'\\b\\d+\\b', r' ', s) # Replace sequences of numbers with a single token\n",
    "    return s\n",
    "\n",
    "#train_data_clean=train_data['full_text'].apply(lambda s:pre_proccess(s))\n",
    "#dev_data_clean=dev_data['full_text'].apply(lambda s: pre_proccess(s))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function extract the seasonality information from our post metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.timeanddate.com/calendar/aboutseasons.html\n",
    "def ts_to_season(month):\n",
    "    if month>=3 and month<=5:\n",
    "        return \"spring\"\n",
    "    elif month>=6 and month <=8:\n",
    "        return \"summer\"\n",
    "    elif month>=9 and month <=11:\n",
    "        return \"fall\"\n",
    "    else:\n",
    "        return \"winter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features for both train and dev set to leverage the metadata provided within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_ft_mat(train_data):\n",
    "    feat_mat=pd.DataFrame()\n",
    "    \n",
    "    # Extract temporal features\n",
    "    feat_mat['hour_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.hour\n",
    "    feat_mat['day_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.day\n",
    "    feat_mat['day_request']=feat_mat['day_request'].apply(lambda x: 0 if x<16 else 1)\n",
    "    feat_mat['season_request']=pd.to_datetime(train_data['unix_timestamp_of_request_utc'],unit = 's').dt.month\n",
    "    feat_mat['season_request']=feat_mat['season_request'].apply(ts_to_season)\n",
    "    feat_mat['is_spring']=feat_mat['season_request'].apply(lambda x: 1 if x=='spring' else 0)\n",
    "    feat_mat['is_summer']=feat_mat['season_request'].apply(lambda x: 1 if x=='summer' else 0)\n",
    "    feat_mat['is_fall']=feat_mat['season_request'].apply(lambda x: 1 if x=='fall' else 0)\n",
    "    feat_mat['is_winter']=feat_mat['season_request'].apply(lambda x: 1 if x=='winter' else 0)\n",
    "    del feat_mat['season_request']\n",
    "    \n",
    "    # Extract post popularity features\n",
    "    feat_mat['first_post']=np.log(train_data['requester_days_since_first_post_on_raop_at_request']+1)\n",
    "    feat_mat['upvotes_minus_downvotes']=train_data['requester_upvotes_minus_downvotes_at_request']\n",
    "    feat_mat['upvotes_plus_downvotes_at_request']=np.log(train_data['requester_upvotes_plus_downvotes_at_request']+1)\n",
    "    upvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request'] + row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    downvotes=train_data.apply(lambda row: (row['requester_upvotes_plus_downvotes_at_request']- row['requester_upvotes_minus_downvotes_at_request'])/2,axis=1)\n",
    "    feat_mat['upvotes']=upvotes\n",
    "    feat_mat['vote_ratio']=upvotes/(upvotes+downvotes+1)\n",
    "    \n",
    "    # Extract requester features\n",
    "    feat_mat['req_age']=np.log(train_data['requester_account_age_in_days_at_request']+1)\n",
    "    feat_mat['num_subs']=np.log(train_data['requester_number_of_subreddits_at_request']+1)\n",
    "    feat_mat['num_posts']=np.log(train_data['requester_number_of_posts_at_request']+1)\n",
    "    feat_mat['pizza_activity']=np.log(train_data['requester_number_of_posts_on_raop_at_request']+1)\n",
    "    feat_mat['pizza_comments']=np.log(train_data['requester_number_of_comments_in_raop_at_request']+1)\n",
    "    feat_mat['community_age'] = (pd.to_datetime(train_data['unix_timestamp_of_request_utc'],utc = True, unit = 's') - \\\n",
    "                                pd.to_datetime('2010-12-8', format='%Y-%m-%d')).astype('timedelta64[D]')\n",
    "    feat_mat['community_age'] = (feat_mat['community_age'] * 10./feat_mat.community_age.max()).astype(int)\n",
    "    \n",
    "    #feat_mat['karma']=(train_data['requester_upvotes_minus_downvotes_at_request']* 10.\\\n",
    "                                         #/train_data.requester_upvotes_minus_downvotes_at_request.max()).astype(int)\n",
    "\n",
    "    #feat_mat['posted_in_raop_before']= (train_data['requester_number_of_posts_on_raop_at_request'] > 0).astype(int)\n",
    "    \n",
    "    #feat_mat['posted_before']= (train_data['requester_number_of_posts_at_request'] > 0).astype(int)\n",
    "    \n",
    "    \n",
    "    # Extract post features\n",
    "    feat_mat['len_request']=np.log(train_data['request_text_edit_aware'].apply(len)+1)\n",
    "    feat_mat['len_title']=np.log(train_data['request_title'].apply(len)+1)\n",
    "    feat_mat['reciprocity'] = train_data['full_text'].apply(lambda x:1 if re.search(\"repay|pay.+back|pay.+forward|return.+favor\", x) \n",
    "                                               else 0)\n",
    "    feat_mat['image_in_text'] = train_data['full_text'].str.contains('imgur.com|.jpg|.png|.jpeg', case=False).apply(lambda x: 1 if x else 0)\n",
    "    feat_mat['politeness'] = train_data['full_text'].apply(lambda x: 1 if re.search(\"thank|appreciate|advance\", x) else 0)\n",
    "    \n",
    "    # Extract narrative features\n",
    "    craving = re.compile(r'(friend|party|birthday|boyfriend|girlfriend|date|drinks|drunk|wasted|invite|invited|celebrate|celebrating|game|games|movie|beer|crave|craving)', re.IGNORECASE)\n",
    "    family = re.compile(r'(husband|wife|family|parent|parents|mother|father|mom|mum|son|dad|daughter)', re.IGNORECASE)\n",
    "    job = re.compile(r'(job|unemployment|employment|hire|hired|fired|interview|work|paycheck)', re.IGNORECASE)\n",
    "    money = re.compile(r'(money|bill|bills|rent|bank|account|paycheck|due|broke|bills|deposit|cash|dollar|dollars|bucks|paid|payed|buy|check|spent|financial|poor|loan|credit|budget|day|now| \\\n",
    "        time|week|until|last|month|tonight|today|next|night|when|tomorrow|first|after|while|before|long|hour|Friday|ago|still|due|past|soon|current|years|never|till|yesterday|morning|evening)', re.IGNORECASE)\n",
    "    student = re.compile(r'(college|student|university|finals|study|studying|class|semester|school|roommate|project|tuition|dorm)', re.IGNORECASE)\n",
    "    #gratitude = re.compile(r'(thank|thanks|thankful|appreciate|grateful|gratitude|advance)', re.IGNORECASE)\n",
    "    feat_mat['money'] = train_data['full_text'].apply(lambda x: len(money.findall(x))/len(x.split()))\n",
    "    feat_mat['job'] = train_data['full_text'].apply(lambda x: len(job.findall(x))/len(x.split()))\n",
    "    feat_mat['student'] = train_data['full_text'].apply(lambda x: len(student.findall(x))/len(x.split()))\n",
    "    feat_mat['family'] = train_data['full_text'].apply(lambda x: len(family.findall(x))/len(x.split()))\n",
    "    feat_mat['craving'] = train_data['full_text'].apply(lambda x: len(craving.findall(x))/len(x.split()))\n",
    "    #feat_mat['gratitude'] = train_data['full_text'].apply(lambda x: len(gratitude.findall(x))/len(x.split()))   \n",
    "    return feat_mat\n",
    "\n",
    "feat_mat=construct_ft_mat(train_data)\n",
    "dev_mat=construct_ft_mat(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert dataframe to a numpy-array representation.\n",
    "t_mat=feat_mat.as_matrix()\n",
    "d_mat=dev_mat.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract text features from the post text by generating features from words\n",
    "vectorizer = TfidfVectorizer(min_df=5,ngram_range=(1,2), preprocessor=pre_proccess,stop_words='english',norm='l2',sublinear_tf=True) \n",
    "train_bag_of_words = vectorizer.fit_transform(train_data['full_text'])\n",
    "dev_bag_of_words = vectorizer.transform(dev_data['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232, 873)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LSVC(C=.85, penalty=\"l1\", dual=False,random_state=42).fit(train_bag_of_words,train_labels)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "\n",
    "X_new = model.transform(train_bag_of_words)\n",
    "print(X_new.shape)\n",
    "\n",
    "d_new=model.transform(dev_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_data['full_text'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train_data['full_text'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LDA(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Topics:\n",
      "\n",
      "Topic 0:\n",
      "ve help work time know like going little family thanks\n",
      "Topic 1:\n",
      "pizza craving like send buy want hut ll random free\n",
      "Topic 2:\n",
      "student college finals studying school poor kid appreciate students university\n",
      "Topic 3:\n",
      "http com imgur jpg www reddit proof comments pic picture\n",
      "Topic 4:\n",
      "pay forward ll promise paycheck soon check rent money thanks\n",
      "Topic 5:\n",
      "job lost new got celebrate paycheck months moved recently started\n",
      "Topic 6:\n",
      "hungry im help pretty dont nc ky right favor sure\n",
      "Topic 7:\n",
      "love pizza usa thanks forever night hot starving finals guys\n",
      "Topic 8:\n",
      "food money house starving stamps appreciated month days left ran\n",
      "Topic 9:\n",
      "really use appreciate right pizza thanks pick don sick haven\n",
      "Topic 10:\n",
      "tonight dinner kids family help pizza daughter uk thanks night\n",
      "Topic 11:\n",
      "day today make work long brighten lunch home bad bed\n",
      "Topic 12:\n",
      "birthday today celebrate party family girlfriend spent awesome enjoy amp\n",
      "Topic 13:\n",
      "ramen noodles eating past living weeks tired ve sick days\n",
      "Topic 14:\n",
      "paid friday week till favor don return rent account help\n",
      "Topic 15:\n",
      "broke car pretty im payday flat appreciated completely title help\n",
      "Topic 16:\n",
      "tomorrow night exams till morning thanks hi pregnant paid dominos\n",
      "Topic 17:\n",
      "eat house want thing friend today ohio don dogs days\n",
      "Topic 18:\n",
      "just story sob want moved got don new sad poor\n",
      "Topic 19:\n",
      "need help make ca young im single meat just payday\n",
      "\n",
      "LDA Topics:\n",
      "\n",
      "Topic 0:\n",
      "az friday dont boys electricity drunk help turned mother roommate\n",
      "Topic 1:\n",
      "pizza just help really food money broke love week don\n",
      "Topic 2:\n",
      "mom friend pizza just dad years family house home help\n",
      "Topic 3:\n",
      "lt gt happens college bad literally great 19 horrible just\n",
      "Topic 4:\n",
      "birthday celebrate party wondering til friday information glad sadly unemployed\n",
      "Topic 5:\n",
      "closed campus missed dining day break early london busy exam\n",
      "Topic 6:\n",
      "given texas tip payed video hell receive youtube sort comments\n",
      "Topic 7:\n",
      "boyfriend shot prove worth sub figured folks wrong jobless kind\n",
      "Topic 8:\n",
      "verification order code free paypal fixed gluten domino chicken makes\n",
      "Topic 9:\n",
      "loan went needed said food money ve lady didn fridge\n",
      "Topic 10:\n",
      "job ve pay just money food time got months month\n",
      "Topic 11:\n",
      "tuesday losing 50 car believe goes couple pictures delicious people\n",
      "Topic 12:\n",
      "homeless fiancee shelter inside hot day country uk outside bring\n",
      "Topic 13:\n",
      "pie young michigan slice virginia far tonight usa pizza gratitude\n",
      "Topic 14:\n",
      "minutes dont start say starts diet personal gluten com celebrate\n",
      "Topic 15:\n",
      "grocery wallet shopping 17 oh hours gas access 100 week\n",
      "Topic 16:\n",
      "want write song draw nyc choice pizza ll starving austin\n",
      "Topic 17:\n",
      "help just delicious pizza car leave pay little today nice\n",
      "Topic 18:\n",
      "pregnant anybody exams sadly movie wanna husband weeks budget wonderful\n",
      "Topic 19:\n",
      "com http pizza imgur www reddit jpg picture game post\n"
     ]
    }
   ],
   "source": [
    "# Derived topics with the top 10 words in each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "print(\"NMF Topics:\\n\")\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print(\"\\nLDA Topics:\\n\")\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the derived topics from NMF and LDA above, NMF seems to find more meaningful and cohesive topics compared to LDA. To gain a better understanding of the topic, we can try displaying the top documents in a topic as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "help food just money\n",
      "Hey there, I am going hungry and would really appreciate a pizza from anyone willing. I'll give a little story about what has led me to be extremely poor (£9 to last until the end of August).\n",
      "\n",
      "I went to university this year, and moved out of my parent's home to forge a new life in Peckham, which is when everything started going horribly wrong. I hated my course, the people on my course seemingly went out of their way to make me feel uncomfortable (I'm quite shy, and very insecure).\n",
      "\n",
      "For years now, I've been fighting my demons, keeping them just below the surface - I've always felt like I was just about floating, reading to sink at any moment. Around Christmas, I did just that - sank like a stone. I fell into a major depressive episode that last months, culminating in what I can only describe as a mental breakdown over Easter. I became dependent on weed, never leaving my room or spending time sober other than to go to work on Sundays, and then I'd come home and get high again. Obviously, I dropped out of university.\n",
      "\n",
      "This, obviously, was a massive waste of money. It also didn't help that I'd lent one of my now ex-housemates £100 to help him pay his rent, which I know now that I will never see again (we live and learn). I have a £970 overdraft that is all gone, and although I've at least paid my rent for this month, I won't be getting paid until the end of August. I also have to pay a week worth of council tax (only £30 but too much for me) for the week before I enrolled, or I'll have a court date, and £40 of library fees tomorrow, but luckily my mum is bailing me out for that (she already bailed me out of last month's rent, so I already owe her £500).\n",
      "\n",
      "I've had to start working full time, eat butter sandwiches at work because it is all I can afford, and improvise for dinner (or just not eat). I'm extremely skinny anyway, and cannot afford this weight loss.\n",
      "\n",
      "I spend four hours commuting a day (I work in West London), come home, eat, sleep, get up at 6am to go to work again...I feel very lonely at the moment, and a cheesy, greasy pizza with meat or something would go a long way to providing me with a little comfort at the moment.\n",
      "\n",
      "Sorry for spilling this all out, I only meant to give a short sum out, but have ended up providing you with an essay! I'm not known for being concise...\n",
      "\n",
      "But yes, a pizza would be so very appreciated, and if the offerer wanted, I can return the pizza favour once I get paid. I would just like some meat and cheese in my life right now haha. Thanks for listening &lt;3 [REQUEST] London, UK  Extremely poor, tired of butter sandwiches!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2058",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7de15d83fdec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mno_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mno_top_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmf_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-7de15d83fdec>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(H, W, feature_names, documents, no_top_words, no_top_documents)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtop_doc_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mno_top_documents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_doc_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# NMF is able to use tf-idf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arthur/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arthur/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2428\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2429\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4363)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4046)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13913)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13857)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2058"
     ]
    }
   ],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    'The display_topics method prints out a numerical index as the topic name, prints the top words in the \\\n",
    "    topic and then prints the top documents in the topic. The top words and top documents have the highest \\\n",
    "    weights in the returned matrices. The argsort() method is used to sort the row or column of the matrix \\\n",
    "    and returns the indexes for the cells that have the highest weights in order.'\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_data['full_text'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train_data['full_text'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LDA(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "no_top_words = 4\n",
    "no_top_documents = 4\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, train_data['full_text'], no_top_words, no_top_documents)\n",
    "display_topics(lda_H, lda_W, tf_feature_names, train_data['full_text'], no_top_words, no_top_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_lda = CountVectorizer(min_df=10,ngram_range=(1,1), preprocessor=pre_proccess,stop_words='english') \n",
    "lda_bag_of_words = vectorizer_lda.fit_transform(train_data['full_text'])\n",
    "lda_devbag_of_words = vectorizer_lda.transform(dev_data['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1580)\n"
     ]
    }
   ],
   "source": [
    "# LDA tells us what topics are present in any given document by observing all the words \n",
    "# in it and producing a topic distribution\n",
    "\n",
    "lda = LDA(n_components = 3, learning_method=\"batch\", max_iter=30,learning_decay=.7, random_state=42)\n",
    "train_topics = lda.fit_transform(lda_bag_of_words)\n",
    "print(lda.components_.shape)\n",
    "\n",
    "dev_topics=lda.transform(lda_devbag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_data['full_text'])\n",
    "dev_tfidf = tfidf_vectorizer.transform(dev_data['full_text'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 3\n",
    "\n",
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_train = nmf_model.transform(tfidf)\n",
    "nmf_dev = nmf_model.transform(dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_new=hstack([X_new,t_mat,nmf_train,train_topics])\n",
    "dev_new=hstack([d_new,d_mat,nmf_dev,dev_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.585806\ttrain-auc:0.862849\n",
      "[1]\teval-auc:0.592023\ttrain-auc:0.927217\n",
      "[2]\teval-auc:0.606472\ttrain-auc:0.964603\n",
      "[3]\teval-auc:0.610724\ttrain-auc:0.976458\n",
      "[4]\teval-auc:0.622743\ttrain-auc:0.983359\n",
      "[5]\teval-auc:0.628014\ttrain-auc:0.986938\n",
      "[6]\teval-auc:0.62465\ttrain-auc:0.98723\n",
      "[7]\teval-auc:0.629683\ttrain-auc:0.986383\n",
      "[8]\teval-auc:0.634457\ttrain-auc:0.987143\n",
      "[9]\teval-auc:0.637233\ttrain-auc:0.989155\n",
      "[10]\teval-auc:0.641826\ttrain-auc:0.990336\n",
      "[11]\teval-auc:0.645592\ttrain-auc:0.99194\n",
      "[12]\teval-auc:0.650728\ttrain-auc:0.992973\n",
      "[13]\teval-auc:0.65206\ttrain-auc:0.99374\n",
      "[14]\teval-auc:0.650152\ttrain-auc:0.994851\n",
      "[15]\teval-auc:0.650761\ttrain-auc:0.995054\n",
      "[16]\teval-auc:0.65646\ttrain-auc:0.995339\n",
      "[17]\teval-auc:0.656859\ttrain-auc:0.995655\n",
      "[18]\teval-auc:0.655592\ttrain-auc:0.996081\n",
      "[19]\teval-auc:0.655485\ttrain-auc:0.996354\n",
      "[20]\teval-auc:0.65868\ttrain-auc:0.996555\n",
      "[21]\teval-auc:0.660966\ttrain-auc:0.996903\n",
      "[22]\teval-auc:0.660366\ttrain-auc:0.997276\n",
      "[23]\teval-auc:0.658898\ttrain-auc:0.997715\n",
      "[24]\teval-auc:0.662455\ttrain-auc:0.997579\n",
      "[25]\teval-auc:0.661028\ttrain-auc:0.997613\n",
      "[26]\teval-auc:0.660789\ttrain-auc:0.997788\n",
      "[27]\teval-auc:0.661386\ttrain-auc:0.9982\n",
      "[28]\teval-auc:0.660888\ttrain-auc:0.99833\n",
      "[29]\teval-auc:0.663051\ttrain-auc:0.998266\n",
      "[30]\teval-auc:0.663437\ttrain-auc:0.998322\n",
      "[31]\teval-auc:0.662907\ttrain-auc:0.998275\n",
      "[32]\teval-auc:0.664757\ttrain-auc:0.99859\n",
      "[33]\teval-auc:0.666451\ttrain-auc:0.998745\n",
      "[34]\teval-auc:0.664054\ttrain-auc:0.998732\n",
      "[35]\teval-auc:0.666221\ttrain-auc:0.998924\n",
      "[36]\teval-auc:0.667171\ttrain-auc:0.999018\n",
      "[37]\teval-auc:0.667438\ttrain-auc:0.999155\n",
      "[38]\teval-auc:0.668236\ttrain-auc:0.999146\n",
      "[39]\teval-auc:0.668269\ttrain-auc:0.999266\n",
      "[40]\teval-auc:0.669165\ttrain-auc:0.999292\n",
      "[41]\teval-auc:0.668594\ttrain-auc:0.999388\n",
      "[42]\teval-auc:0.668549\ttrain-auc:0.999443\n",
      "[43]\teval-auc:0.669926\ttrain-auc:0.999474\n",
      "[44]\teval-auc:0.670683\ttrain-auc:0.999517\n",
      "[45]\teval-auc:0.671859\ttrain-auc:0.999445\n",
      "[46]\teval-auc:0.672418\ttrain-auc:0.999492\n",
      "[47]\teval-auc:0.670789\ttrain-auc:0.999513\n",
      "[48]\teval-auc:0.671612\ttrain-auc:0.999597\n",
      "[49]\teval-auc:0.672163\ttrain-auc:0.999607\n",
      "[50]\teval-auc:0.672985\ttrain-auc:0.999645\n",
      "[51]\teval-auc:0.672105\ttrain-auc:0.999688\n",
      "[52]\teval-auc:0.672599\ttrain-auc:0.999744\n",
      "[53]\teval-auc:0.673947\ttrain-auc:0.999788\n",
      "[54]\teval-auc:0.674276\ttrain-auc:0.999802\n",
      "[55]\teval-auc:0.674087\ttrain-auc:0.999805\n",
      "[56]\teval-auc:0.672344\ttrain-auc:0.999838\n",
      "[57]\teval-auc:0.673067\ttrain-auc:0.999831\n",
      "[58]\teval-auc:0.673701\ttrain-auc:0.999827\n",
      "[59]\teval-auc:0.673997\ttrain-auc:0.999847\n",
      "[60]\teval-auc:0.676439\ttrain-auc:0.999859\n",
      "[61]\teval-auc:0.676612\ttrain-auc:0.999883\n",
      "[62]\teval-auc:0.676604\ttrain-auc:0.999889\n",
      "[63]\teval-auc:0.677599\ttrain-auc:0.999888\n",
      "[64]\teval-auc:0.678676\ttrain-auc:0.999903\n",
      "[65]\teval-auc:0.67926\ttrain-auc:0.999906\n",
      "[66]\teval-auc:0.679457\ttrain-auc:0.999914\n",
      "[67]\teval-auc:0.678783\ttrain-auc:0.999927\n",
      "[68]\teval-auc:0.677936\ttrain-auc:0.999933\n",
      "[69]\teval-auc:0.678002\ttrain-auc:0.999943\n",
      "[70]\teval-auc:0.67787\ttrain-auc:0.999942\n",
      "[71]\teval-auc:0.678816\ttrain-auc:0.99995\n",
      "[72]\teval-auc:0.680411\ttrain-auc:0.999955\n",
      "[73]\teval-auc:0.679959\ttrain-auc:0.999959\n",
      "[74]\teval-auc:0.679729\ttrain-auc:0.999965\n",
      "[75]\teval-auc:0.678964\ttrain-auc:0.999971\n",
      "[76]\teval-auc:0.680058\ttrain-auc:0.999974\n",
      "[77]\teval-auc:0.680337\ttrain-auc:0.999974\n",
      "[78]\teval-auc:0.681036\ttrain-auc:0.999976\n",
      "[79]\teval-auc:0.680576\ttrain-auc:0.999979\n",
      "[80]\teval-auc:0.680757\ttrain-auc:0.999986\n",
      "[81]\teval-auc:0.681225\ttrain-auc:0.999989\n",
      "[82]\teval-auc:0.681077\ttrain-auc:0.999992\n",
      "[83]\teval-auc:0.681143\ttrain-auc:0.999993\n",
      "[84]\teval-auc:0.682434\ttrain-auc:0.999994\n",
      "[85]\teval-auc:0.682664\ttrain-auc:0.999995\n",
      "[86]\teval-auc:0.683166\ttrain-auc:0.999997\n",
      "[87]\teval-auc:0.683972\ttrain-auc:0.999996\n",
      "[88]\teval-auc:0.68352\ttrain-auc:0.999996\n",
      "[89]\teval-auc:0.684441\ttrain-auc:0.999995\n",
      "[90]\teval-auc:0.684729\ttrain-auc:0.999996\n",
      "[91]\teval-auc:0.685099\ttrain-auc:0.999997\n",
      "[92]\teval-auc:0.685469\ttrain-auc:0.999997\n",
      "[93]\teval-auc:0.68616\ttrain-auc:0.999997\n",
      "[94]\teval-auc:0.686127\ttrain-auc:0.999998\n",
      "[95]\teval-auc:0.687418\ttrain-auc:0.999998\n",
      "[96]\teval-auc:0.686735\ttrain-auc:0.999998\n",
      "[97]\teval-auc:0.686521\ttrain-auc:0.999998\n",
      "[98]\teval-auc:0.686883\ttrain-auc:0.999998\n",
      "[99]\teval-auc:0.687615\ttrain-auc:0.999997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create dmatrices\n",
    "dtrain = xgb.DMatrix(f_new, train_labels)\n",
    "dtest = xgb.DMatrix(dev_new\n",
    "                         , dev_labels)\n",
    "\n",
    "#booster parameter\n",
    "param = {'max_depth':15, 'eta': .015, 'silent': 1, 'objective': 'binary:logistic'\n",
    "         , 'scale_pos_weight': 3.06,'max_delta_step':1,'subsample':.9,'seed':42}#9 depth if sublin false\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "#specify validation set to watch performance\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "#train model\n",
    "num_round = 100\n",
    "bst = xgb.train(param.items(), dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a5c1d8fc7bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentiment_data_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentiment_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentiment_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmetrix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msentiment_data_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmetrix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrix' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_metrics = ['neg', 'pos'] #compound metrics produce negative numbers which we can't use\n",
    "sentiment_data_train = pd.DataFrame()\n",
    "sentiment_data_dev = pd.DataFrame()\n",
    "for metrics in sentiment_metrics:\n",
    "    sentiment_data_train['full_text'+ metrics] = train_data['full_text'].apply(lambda x: sia.polarity_scores(x)[metrics])\n",
    "    sentiment_data_dev['full_text'+ metrics] = dev_data['full_text'].apply(lambda x: sia.polarity_scores(x)[metrics])\n",
    "\n",
    "sent_mat_train = sentiment_data_train.as_matrix()\n",
    "sent_mat_dev =sentiment_data_dev.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
